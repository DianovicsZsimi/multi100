---
title: "General descriptives"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)

source(here::here("R/utils.R"))

processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

# Add number of evaluations per analysis
# TODO: check whether this is implemented in raw processed
peer_eval <-
  peer_eval %>% 
  group_by(paper_id, analyst_id) %>% 
  mutate(n_evaluations = n()) %>% 
  ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

processed <-
  processed %>% 
  mutate(
    # Transforming the timestamp to date type from character
    task1_timestamp = lubridate::ymd_hms(task1_timestamp)
    )
```

```{r include=FALSE}
# Check if the analyst_id's are always unique to one person.
processed %>%
  distinct(first_name, last_name, analyst_id) %>%  
  group_by(first_name, last_name) %>% 
  mutate(n_analyst_id = n()) %>% 
  arrange(desc(n_analyst_id))
```

## General

```{r include=FALSE}
# TODO: somebody check the names for typos that could mess with the count
analyst_signed_up <- 
  all_people %>% 
  mutate(
    first_name = tolower(first_name),
    last_name = tolower(last_name)
  ) %>% 
  distinct(first_name, last_name, .keep_all = T) %>% 
  filter(disclosure_agreement == "I agree")

analyst_submitted <-
  processed %>% 
  distinct(analyst_id) %>% 
  nrow()
```

As a response to our recruitment call, `r nrow(analyst_signed_up)` researchers signed up to participate in our study. Out of these volunteers, `r analyst_submitted` signed up to analyse at least one datasets and submitted their work by the deadline or an extended deadline.

```{r include=FALSE}
n_analysis <-
  processed %>% 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of co-analysts as some co-analysts volunteered to analyse more than one dataset.

```{r include=FALSE}
n_failed_peer <-
  processed %>% 
  filter(!peer_eval_pass) %>% 
  nrow()
```

Out of the submitted analyses, \_\ were withdrawn, and `r nrow(filter(processed, !peer_eval_pass))` were omitted from the summary analysis their analysis failed the peer evaluation.

```{r include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  filter(peer_eval_pass)
```

```{r include=FALSE}
final_n_analyst <-
  processed %>% 
  distinct(analyst_id) %>% 
  nrow()
```

As a result, we ended up with `r nrow(processed)` re-analyses, submitted by `r final_n_analyst` co-analysts.

## Task 1 Survey results

```{r include=FALSE}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position_grouped)

position <-
  processed %>%
  dplyr::select(analyst_id, paper_id, current_position_grouped, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(current_position_grouped) %>% 
  rename(position = current_position_grouped)
```

Out of all the co-analysts who submitted their work by the deadline, there were `r filter(position, position == "Professor") %>% pull(n)` professors, `r filter(position, position == "Associate Professor") %>% pull(n)` associate professors, `r filter(position, position == "Post-Doc Researcher") %>% pull(n)` post-doctoral researchers, `r filter(position, position == "Doctoral Student") %>% pull(n)` doctoral students, `r filter(position, position == "Other academic/research position") %>% pull(n)` from other academic/research positions, and `r filter(position, position == "Other") %>% pull(n)` from other positions. 

```{r include=FALSE}
check_diff_response(processed, analyst_id, gender)

gender <-
  processed %>%
  dplyr::select(analyst_id, paper_id, gender, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(gender)
```

The gender distribution of the co-analysts is as follows: `r filter(gender, gender == "Female") %>% pull(n)` female, `r filter(gender, gender == "Male") %>% pull(n)` male, `r filter(gender, gender == "Non-binary") %>% pull(n)` other, and `r filter(gender, gender == "Prefer not to say") %>% pull(n)` didn't want to respond to this question.

```{r include=FALSE}
check_diff_response(processed, analyst_id, age)

age <-
  processed %>%
  dplyr::select(analyst_id, paper_id, age, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) |> 
  # Filter erroneous data
  filter(age != "00") |> 
  mutate(age = as.numeric(age))
  
age_group <-
  age %>% 
  mutate(
    age_group = case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    ),
    age_group = factor(age_group, levels = c("young", "middle", "old"))
    ) %>% 
  count(age_group) |> 
  complete(age_group, fill = list(n = 0))
```

The age distribution of the co-analysts is depicted in @fig-age-plot. `r filter(age_group, age_group == "young") %>% pull(n)` Young adults (-39 years); `r filter(age_group, age_group == "middle") %>% pull(n)` middle-ages adults (40-59 years); and no old adults (60- years).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-age-plot
#| fig-cap: "The figure shows the distribution of the analysts' age. When an analyst submitted more than one re-analysis with more than a year apart, we only kept their age at the time of their first submission. Moreover, one analyst was excluded because they did not disclose their age."
age_plot <- 
  age %>%
  ggplot() +
  aes(x = age) +
  geom_histogram(binwidth = 1) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 40)) +
  scale_x_continuous(
    # limits = c(20, 55),
    # breaks = c(20, 30, 40, 50, 60),
    # labels = c("20", "30", "40", "50", "60")
  ) +
  labs(x = "Age",
       y = "Count") +
  theme(
    panel.grid = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line()
  )

ggsave(here::here("figures/demographic_age_plot.jpg"), age_plot, dpi = 300)

age_plot
```

```{r include=FALSE}
check_diff_response(processed, analyst_id, education_level)

education <-
  processed %>%
  dplyr::select(analyst_id, paper_id, education_level, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(education_level) %>% 
  rename(education = education_level)
```

Regarding the highest level of education, `r filter(education, education == "") %>% pull(n)` co-analysts had Bachelor's degree or equivalent, `r filter(education, education == "") %>% pull(n)` Master's degree or equivalent, `r filter(education, education == "") %>% pull(n)` had Doctoral degree or equivalent; and `r filter(education, education == "") %>% pull(n)` reported other degree. In case the analysts completed more than one re-analysis and they advanced in their studies by the time of their second analysis, we only kept their first response for this comparison.

```{r include=FALSE}
check_diff_response(processed, analyst_id, country_of_residence)

country_data <- raster::ccodes()

country <- 
  processed %>%
  dplyr::select(analyst_id, paper_id, country_of_residence, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(country_of_residence) %>% 
  rename(region = country_of_residence) |> 
  # Modify country names to fit the worldmap data
  mutate(
    subregion = case_when(
      region == "Hong Kong (China)" ~ "Hong Kong",
      TRUE ~ NA_character_
    ),
    region = case_when(
      region == "Hong Kong (China)" ~ "China",
      region == "United States" ~ "USA",
      region == "United Kingdom" ~ "UK",
      TRUE ~ region
    ),
    continent = countrycode(region, "country.name", "continent"),
    iso3_code = countrycode(region, "country.name", "iso3c")
  ) |> 
  left_join(dplyr::select(country_data, ISO3, UNREGION1), by = c("iso3_code" = "ISO3"))

continent <-
  country |> 
  group_by(continent) |> 
  summarise(N = sum(n))

region <-
  country |> 
  group_by(UNREGION1) |> 
  summarise(N = sum(n))
```

The country of residence of the co-analysts is shown on the map on @fig-country-plot. Regarding continents, `r filter(continent, continent == "Africa") |> pull(N)` co-analysts were from Africa, `r filter(continent, continent == "Asia") |> pull(N)` from Asia, `r filter(continent, continent == "Oceania") |> pull(N)` from Oceania, `r filter(continent, continent == "Europe") |> pull(N)` from Europe, `r filter(region, UNREGION1 == "Northern America") |> pull(N)` from North America, `r filter(region, UNREGION1 %in% c("Central America", "South America")) |> summarise(sum(N)) |> pull("sum(N)")` from South America.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-country-plot
#| fig-cap: "The figure shows the analysts' country of residence. When an analyst submitted more than one re-analysis and they moved between the submissions, we only kept their first response."

# TODO: This is an ugly solution ask someone for a better one
# use coaleace()
world_map <- 
  map_data("world") |> 
  mutate(
    subregion = case_when(
      subregion == "Hong Kong" ~ subregion,
      TRUE ~ NA_character_
    )
  )

country_map <- left_join(world_map, country, by = c("region", "subregion"))

country_map_plot <- 
  country_map |> 
  ggplot() +
  aes(x = long, y = lat, group = group, fill = n) +
  geom_polygon(color = "white", linewidth = 0.2) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Number of\nanalyst") +
  theme_void()

ggsave(here::here("figures/demographic_country_plot.jpg"), country_map_plot, dpi = 300)

country_map_plot
```

```{r include=FALSE}
check_diff_response(processed, analyst_id, primary_discipline)

analyst_discipline <- 
  processed %>%
  dplyr::select(analyst_id, paper_id, primary_discipline, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  calculate_percentage(primary_discipline) %>%
  rename(discipline = primary_discipline) |> 
  dplyr::arrange(desc(percentage))
```

We asked the co-analysts which discipline is the closest to their research area. The following Table summarises the distribution of their disciplinary orientation. Co-analysts from `r slice(analyst_discipline, 1) |> pull(discipline)` and `r slice(analyst_discipline, 2) |> pull(discipline)` disciplines participated in the highest ratio in this study.

```{r, echo=FALSE}
# tbl-discipline
# "Distribution of the Analysts' Primary Discipline"

analyst_discipline |>
  dplyr::select(discipline, n, percentage) |>
  dplyr::rename(Discipline = discipline,
                Count = n,
                Percentage = percentage) |>
  gt() |>
  tab_style(style = cell_text(weight = "bold"),
            locations = cells_column_labels()) |>
  tab_footnote(
    "Note: Whenever the respondents provided more than one field we only kept their first responses."
  )
```

```{r include=FALSE}
check_diff_response(processed, analyst_id, years_of_experience)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-experience-years-plot
#| fig-cap: "The figure shows the analysts' years of experience with data analysis. When an analyst submitted more than one re-analysis and a year passed between the responses we only kept their first response."
analyst_experience_years_data <-
  processed %>%
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  # Dropped because of faulty response
  filter(analyst_id != "RTX71")

analyst_experience_years_plot <-
  analyst_experience_years_data %>% 
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience with data analysis",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_experience_years_plot.jpg"), analyst_experience_years_plot, dpi = 300)

analyst_experience_years_plot
```

The distribution of the years of experience with data analysis is depicted on @fig-experience-years-plot. The median time of experience with data analysis was `r median(analyst_experience_years_data$years_of_experience)` years among our co-analysts.

```{r include=FALSE}
check_diff_response(processed, analyst_id, analysis_frequency)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-analysis-frequency
#| fig-cap: "The figure shows how regularly the analysts perform data analysis. When an analyst completed multiple re-analyses we kept all their responses for this figure."

analysis_frequency_count <-
  processed %>% 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count %>%
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) %>%
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of doing data analysis",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_analysis_frequency_plot.jpg"), analysis_frequency_plot, dpi = 300)

analysis_frequency_plot
```

We asked our co-analysts of how regularly they perform data analysis. @fig-analysis-frequency. shows that the most frequent category was `r filter(analysis_frequency_count, n == max(n)) %>% pull(analysis_frequency)`.

```{r echo=FALSE, message=FALSE}
#| label: fig-self-rating-plot
#| fig-cap: "The figure shows the analysts' self-rated level of expertise in data analysis. When an analyst submitted more than one re-analysis we only kept their first response."

expertise_self_rating_data <-
  processed %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>%
  count(expertise_self_rating)

expertise_self_rating_plot <-
  expertise_self_rating_data %>%
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(
      expertise_self_rating,
      c("1\n(Beginner)",
        as.character(2:9),
        "10\n(Expert)")
    )
  ) %>%
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-rated expertise of data analysis",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_expertise_self_rating_plot.jpg"), expertise_self_rating_plot, dpi = 300)

expertise_self_rating_plot
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution on @fig-self-rating-plot shows that the most prevalent answer was `r filter(expertise_self_rating_data, n == max(n)) %>% pull(expertise_self_rating)` .

```{r include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

All together, `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(N)`) co-analysts indicated that they were familiar with the paper that the provided dataset belongs to before beginning their work on the project.

```{r include=FALSE}
processed %>% 
  count(communication_check)
```

No co-analysts reported that they communicated about the details of their analysis with other co-analysts working with the same dataset.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n))
```

We asked the co-analysts what programming language/software/tool they used in their data analysis during Task 1 and Task 2. The following figure indicates that `r slice(software_data, 1) |> pull(software)` (`r slice(software_data, 1) |> pull(percentage)`%), `r slice(software_data, 2) |> pull(software)` (`r slice(software_data, 2) |> pull(percentage)`%), and `r slice(software_data, 3) |> pull(software)` (`r slice(software_data, 3) |> pull(percentage)`%) were the most popular responses. @fig-software shows the distribution of all the responses.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-software
#| fig-cap: "The figure shows which software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported the use of multiple software we kept all their responses for this figure. We only included software that was used by more than 1% of the analysts on the figure."

software_plot <-
  software_data |>
  dplyr::filter(percentage > 1) |>
  dplyr::mutate(software = str_to_title(software)) |>
  ggplot() +
  aes(y = reorder(software, percentage),
      x = percentage) +
  geom_bar(stat = "identity") +
  scale_x_continuous(expand = c(0, 0),
                     labels = scales::percent_format(scale = 1)) +
  labs(x = "Percentage",
       y = "Software") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_software_plot.jpg"), software_plot, dpi = 300)

software_plot
```

## Task 2 Survey results

```{r include=FALSE}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the co-analysts to present one main statistical result, `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(percentage)`% of them (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(n)` our of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(N)`) based their conclusion on p-value and `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(percentage)`% of them (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(n)` our of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(N)`) used Bayes Factor.

```{r include=FALSE}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

A difference in Task 2 compared to Task 1 was that the co-analysts received some constraints for their analysis in order to make them linkable to a single result in the original study. `r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(percentage)` % (`r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(N)`) the co-analysts reported that they had to make additional calculations in the second task. In `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(N)`) the co-analysts indicated that despite the limitations in the instructions, they received the same result in Task 2 and Task 1.

```{r include=FALSE}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(n)` our of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-total-hours
#| fig-cap: "The figure shows the total hours the analyst spent on Task 1 and Task 2. In case an analyst completed multiple re-analyses we kept all their responses for this figure. One response was excluded due to being an outlier."

total_hours_data <-
  processed %>% 
  filter(total_hours != 999)

total_hours_plot <-
  total_hours_data %>%
  ggplot() +
  aes(x = total_hours) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Total hours spent on the analysis",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_total_hours_plot.jpg"), total_hours_plot, dpi = 300)

total_hours_plot
```

The co-analysts were asked to estimate the time they spent to perform Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (Fig @fig-total-hours).

## Peer evaluation
### Peer evaluators

Basic demographic info.

```{r include=FALSE}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval %>% 
  distinct(evaluator_id) %>% 
  inner_join(., processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data %>% 
  count(evaluator_id) %>% 
  arrange(desc(n))
```

Experience with conducting statistical analysis:

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analyst_experience_years_data <-
  peer_evaluator_data %>%
  dplyr::select(evaluator_id, years_of_experience, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data %>%
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_experience_years_plot.jpg"), peer_analyst_experience_years_plot, dpi = 300)

peer_analyst_experience_years_plot
```

Frequency of data analysis:

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analysis_frequency_count <-
  peer_evaluator_data %>% 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <-
  peer_analysis_frequency_count %>%
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) %>%
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of analysis",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_analysis_frequency_plot.jpg"), peer_analysis_frequency_plot, dpi = 300)

peer_analysis_frequency_plot
```

Self-reported expertise in data-analysis:

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_expertise_self_rating_data <-
  peer_evaluator_data %>% 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp) %>%
  count(expertise_self_rating) %>% 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data %>%
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) %>%
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-reported expertise rating",
       y = "Count") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_expertise_self_rating_plot.jpg"), peer_expertise_self_rating_plot, dpi = 300)

peer_expertise_self_rating_plot
```

### Peer evaluations

Nr. of peer evaluations:

```{r include=FALSE}
nrow(peer_eval)
```

Descriptives of peer evaluations.

```{r include=FALSE}
# TODO: should I group the responses by acceptable and unacceptable first?
pipeline_acceptable_data <-
  peer_eval %>% 
  # Dropping analyses with less than 1 evaluation
  filter(n_evaluations > 1) %>% 
  group_by(paper_id, analyst_id) %>% 
  summarise(
    disagree_task1 = n_distinct(task1_pipeline_acceptable) > 1,
    disagree_task2 = n_distinct(task2_pipeline_acceptable) > 1,
  ) %>% 
  ungroup()

task1_pipeline_acceptable_data <- calculate_percentage(pipeline_acceptable_data, disagree_task1)

task2_pipeline_acceptable_data <- calculate_percentage(pipeline_acceptable_data, disagree_task2)
```

For those analyses where there were more than one peer evaluations, for `r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(percentage)`% (`r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(n)` out of `r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(N)`) of the analysis the evaluators disagreed on the analytical pipeline for task 1, and `r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(percentage)`% (`r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(n)` out of `r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(N)`) for task 2.

```{r include=FALSE}
distinct(peer_eval, task1_pipeline_acceptable)

task1_acceptable_summary <-
  peer_eval |>
  mutate(
    is_acceptable = case_when(
      task1_pipeline_acceptable == "(1) Unacceptable" ~ FALSE,
      task1_pipeline_acceptable %in% c(
        "(2) Acceptable but low quality",
        "(3) Acceptable and medium quality",
        "(4) Acceptable and high quality"
      ) ~ TRUE
    )
  ) |>
  calculate_percentage(is_acceptable)

# TODO: rewrite this part for safety
task1_pipline_acceptable_review <-
  peer_eval_review |>
  filter(variable_name == "task1_pipeline_acceptable" &
           !is.na(change_to))
```

`r filter(task1_acceptable_summary, is_acceptable) |> pull(percentage)`% (`r filter(task1_acceptable_summary, is_acceptable) |> pull(n)` out of `r filter(task1_acceptable_summary, is_acceptable) |> pull(N)`) of acceptable analysis pipelines (Task 1) - the outcome of the procedure

`r nrow(task1_pipline_acceptable_review)` of peer elevators' responses we need to adjust for this variable (comment from Marci: we only adjusted the unacceptable responses to acceptable but with low quality I think we can be explicit about that)

```{r include=FALSE}
distinct(peer_eval, task2_pipeline_acceptable)

task2_acceptable_summary <-
  peer_eval |>
  mutate(
    is_acceptable = case_when(
      task2_pipeline_acceptable == "(1) Unacceptable" ~ "unacceptable",
      task2_pipeline_acceptable %in% c(
        "(2) Acceptable but low quality",
        "(3) Acceptable and medium quality",
        "(4) Acceptable and high quality"
      ) ~ "acceptable",
      task2_pipeline_acceptable == "(5) Incomplete or missing analysis" ~ "incomplete",
      TRUE ~ NA_character_
    )
  ) |>
  calculate_percentage(is_acceptable)

# TODO: rewrite this part for safety
task2_pipline_acceptable_review <-
  peer_eval_review |>
  filter(variable_name == "task2_pipeline_acceptable" &
           !is.na(change_to))
```

`r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(percentage)`% (`r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(n)` out of `r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(N)`) analysis pipelines (Task 2) were acceptable, and `r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(percentage)`% (`r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(n)` out of `r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(N)`) analysis pipelines (Task 2) were incomplete

`r nrow(task2_pipline_acceptable_review)` responses of peer evaluations we need to adjust for this variable (comment form marci: again we only modified the unacceptable to acceptable)

```{r include=FALSE}
task1_categorisation_is_accurate_data <-
  peer_eval %>% 
  # Dropping analyses with less than 1 evaluation
  filter(n_evaluations > 1) %>% 
  group_by(paper_id, analyst_id) %>% 
  summarise(
    disagree_task1 = n_distinct(task1_categorisation_is_accurate) > 1
  ) %>% 
  ungroup() %>% 
  calculate_percentage(disagree_task1)
```

For those analyses where there were more than one peer evaluator, `r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(percentage)`% (`r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(n)` out of `r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(N)`) of evaluators disagreed on the adequacy of the conclusions.

```{r include=FALSE}
distinct(peer_eval, task1_categorisation_is_accurate)

task1_categorisation_summary <- 
  peer_eval |> 
  calculate_percentage(task1_categorisation_is_accurate) |> 
  mutate(
    task1_categorisation_is_accurate = stringr::str_remove(task1_categorisation_is_accurate, "\\(\\d+\\)\\s*")
  )

# TODO: rewrite this part for safety
task1_categorisation_review <-
  peer_eval_review |>
  filter(variable_name == "task1_categorisation_is_accurate" &
           !is.na(change_to))
```

`r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(percentage)`% (`r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(n)` out of `r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(N)`) the co-analyst’s self-categorization of the results was adequate.

`r nrow(task1_categorisation_review)` peer evaluations we need to adjust

```{r include=FALSE}
distinct(peer_eval, task1_conclusion_follows_results)

task1_conclusion_summary <- 
  peer_eval |> 
  calculate_percentage(task1_conclusion_follows_results) |> 
  mutate(
    task1_conclusion_follows_results = stringr::str_remove(task1_conclusion_follows_results, "\\(\\d+\\)\\s*")
  )

task1_conclusion_review <-
  peer_eval_review |>
  dplyr::filter(variable_name == "task1_conclusion_follows_results" &
           !is.na(change_to))
```

`r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(percentage)`% (`r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(n)` out of `r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(N)`) conclusions adequately followed from the results of the analysis for Task 1.

\% (x out of y) of cases where the correction of the self-categorization of the conclusion was necessary (comment from marci: this is 0 right now as we discussed, harry will go through them)

Nr. of analytical reproducibility checks:

```{r include=FALSE}
reproducibility_checks_data <-
  peer_eval %>% 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_data %>% 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") %>% 
  summarise(n_reproducibility_checks = sum(n))
```

```{r include=FALSE}
reproducibility_checks_successful <-
  peer_eval %>% 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") %>% 
  mutate(successful = case_when(
    any_code_mismatches %in% c("(2) I tried but didn’t manage to execute it", "(4) I executed it and I found mismatches") ~ FALSE,
    any_code_mismatches == "(3) I executed it and I found no mismatches" ~ TRUE,
    )
  ) %>% 
  calculate_percentage(successful)
```

`r filter(reproducibility_checks_successful, successful) %>% pull(percentage)`% (`r filter(reproducibility_checks_successful, successful) %>% pull(n)` out of `r filter(reproducibility_checks_successful, successful) %>% pull(N)` of the analytical reproducibility checks were successful

## How robust are conclusions published in social sciences to analytical choices?

Do different analysts arrive at the same conclusions as the analysts of the original study?

### Task 1 Survey results

```{r include=FALSE}
# Distinct values of task1_categorisation
distinct(processed, task1_categorisation)
distinct(processed, task1_categorisation_plotting)

conclusions_main_data <- 
  processed %>% 
  rename(categorisation = task1_categorisation_plotting) %>% 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) %>% 
  calculate_conclusion(., simplified_paper_id, categorisation) %>%
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(categorisation == "Same conclusion", percentage, NA), .desc = FALSE, .na_rm = TRUE)
  )

conclusions_main_robustness_data <- calculate_conclusion_robustness(processed, categorization_variable = task1_categorisation_plotting)
```

In Task 1, the co-analysts were asked to conduct any statistical analysis to arrive to a single conclusion. Out of `r nrow(conclusions_main_robustness_data)` re-analysed studies, the conclusions of `r filter(conclusions_main_robustness_data, robust == "Robust") %>% pull(n)` (`r filter(conclusions_main_robustness_data, robust == "Robust") %>% pull(n)`%) remained robust to independent re-analysis, so that all assigned co-analysts arrived at the same conclusion as reported in the article of the original study.

```{r echo=FALSE}
plot_conclusion_robustness(conclusions_main_robustness_data, robust)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-main
#| fig-cap: "The figure shows the histrophic display of the conclusion resulting from the re-analysis of each study."

conclusion_main_plot <-
  plot_conclusion(conclusions_main_data, simplified_paper_id, categorisation) +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 10),
    axis.title.x = element_text(size = 15),
    legend.justification = "left",
    legend.box = "horizontal",
    legend.position = "bottom",
    legend.text = element_text(size = 10)
    )

# Using a hacky solution to move legend under the Y axis
# conclusion_main_plot_wo_legend <- conclusion_main_plot + theme(legend.position = "none")
# legend <- cowplot::get_legend(conclusion_main_plot)
# conclusion_main_plot_w_legend <- cowplot::plot_grid(conclusion_main_plot_wo_legend, legend, nrow = 2, rel_heights = c(1, 0.05), rel_widths = c(1, 5))

ggsave(here::here("figures/conclusion_main_plot.jpg"), conclusion_main_plot, width = 9, height = 11.69, dpi = 300)

conclusion_main_plot
```

@fig-conclusions-main shows the histotrophic display of the different and identical conclusions resulting from the re-analysis of each of the studies.

```{r include=FALSE}
conclusions_analysis_data <- 
  processed %>% 
  rename(categorisation = task1_categorisation_plotting) %>% 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) %>% 
  count(categorisation) %>% 
  ungroup() %>% 
  mutate(
    N = sum(n),
    freq = n / N,
    percentage = round(freq * 100, 2)
  )
```

Across all the studies, `r filter(conclusions_analysis_data, categorisation == "Same conclusion") %>% pull(percentage)` % (`r filter(conclusions_analysis_data, categorisation == "Same conclusion") %>% pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Same conclusion") %>% pull(N)`) of the re-analyses arrived to the same conclusion; `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") %>% pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") %>% pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") %>% pull(N)`) to no effects, and `r filter(conclusions_analysis_data, categorisation == "Opposite effect") %>% pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "Opposite effect") %>% pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Opposite effect") %>% pull(N)`) to opposite effect compared to the original conclusion.

### Robustness

#### Robustness by field

We were interested to see whether these results show a different pattern when inspecting them in different fields. The following figure shows that for the major fields (Psychology, Economics, and Political Science) the pattern were...,

```{r include=FALSE}
conclusions_discipline_data <- 
  processed %>% 
  filter(paper_discipline %in% c("psychology", "economics", "political science")) %>% 
  mutate(paper_discipline = str_to_title(paper_discipline)) %>% 
  rename(categorisation = task1_categorisation_plotting) %>%
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) %>% 
  calculate_conclusion(., paper_discipline, categorisation)

conclusions_discipline_robustness_data <- 
  processed %>% 
  filter(paper_discipline %in% c("psychology", "economics", "political science")) %>% 
  mutate(paper_discipline = str_to_title(paper_discipline)) %>% 
  calculate_conclusion_robustness(., grouping_variable = paper_discipline, categorization_variable = task1_categorisation_plotting)
```

```{r echo=FALSE, message=FALSE}
conclusions_discipline_robustness_plot <- plot_conclusion_robustness(conclusions_discipline_robustness_data, robust, paper_discipline)

ggsave(here::here("figures/conclusions_discipline_robustness_plot.jpg"), conclusions_discipline_robustness_plot, dpi = 300)

conclusions_discipline_robustness_plot
```

```{r echo=FALSE, message=FALSE}
conclusions_discipline_plot <- plot_conclusion(conclusions_discipline_data, paper_discipline, categorisation, with_labels = TRUE) + 
  theme(
    legend.text = element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal"
    )

ggsave(here::here("figures/conclusions_discipline_plot.jpg"), conclusions_discipline_plot, dpi = 300)

conclusions_discipline_plot
```

#### Robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type. The following figure shows that for that...,

```{r include=FALSE}
conclusions_studytype_data <-
  processed %>%
  rename(categorisation = task1_categorisation_plotting) %>%
  mutate(categorisation = fct_relevel(
    categorisation,
    c("Same conclusion", "No effect/inconclusive", "Opposite effect")
  )) %>%
  calculate_conclusion(., experimental_or_observational, categorisation) |>
  mutate(experimental_or_observational = str_to_title(experimental_or_observational))

conclusions_studytype_robustness_data <-
  processed %>%
  calculate_conclusion_robustness(grouping_variable = experimental_or_observational,
                                  categorization_variable = task1_categorisation_plotting) %>%
  ungroup()
```

```{r echo=FALSE, message=FALSE}
conclusions_studytype_robustness_plot <- plot_conclusion_robustness(conclusions_studytype_robustness_data, robust, experimental_or_observational)

ggsave(here::here("figures/conclusions_studytype_robustness_plot.jpg"), conclusions_studytype_robustness_plot, dpi = 300)

conclusions_studytype_robustness_plot
```

```{r echo=FALSE, message=FALSE}
conclusions_studytype_plot <- plot_conclusion(conclusions_studytype_data, experimental_or_observational, categorisation, with_labels = TRUE) + 
  theme(
    legend.text = element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal"
    )

ggsave(here::here("figures/conclusions_studytype_plot.jpg"), conclusions_studytype_plot, dpi = 300)

conclusions_studytype_plot
```

#### Robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts. The following figure shows that for that...,

```{r include=FALSE}
conclusions_expertise_data <-
  processed %>%
  rename(categorisation = task1_categorisation_plotting) %>%
  mutate(
    categorisation = fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    ),
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) %>%
  calculate_conclusion(., expertise_self_rating, categorisation)
```

```{r echo=FALSE}
conclusions_expertise_plot <- plot_conclusion(conclusions_expertise_data, expertise_self_rating, categorisation, with_labels = TRUE) + 
  theme(
    legend.text = element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal"
    )

ggsave(here::here("figures/conclusions_expertise_plot.jpg"), conclusions_expertise_plot, dpi = 300)

conclusions_expertise_plot
```

#### Robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. The following figure shows that for that...,

```{r include=FALSE}
count(processed, familiar_with_paper)

conclusions_familiarity_data <- 
  processed %>% 
  rename(categorisation = task1_categorisation_plotting) %>%
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    familiar_with_paper = as.factor(familiar_with_paper)
    ) %>% 
  calculate_conclusion(., familiar_with_paper, categorisation)
```

```{r echo=FALSE, message=FALSE}
conclusions_familiarity_plot <- 
  plot_conclusion(
    conclusions_familiarity_data,
    familiar_with_paper,
    categorisation,
    with_labels = TRUE,
    y_lab = "Analyst familiarity with the paper") + 
  theme(
    legend.text = element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal"
    )

ggsave(here::here("figures/conclusions_familiarity_plot.jpg"), conclusions_familiarity_plot, dpi = 300)

conclusions_familiarity_plot
```

#### Robustness by the suitability of their self-judged analysis

```{r include=FALSE}
conclusions_suitability_data <- 
  processed %>% 
  rename(categorisation = task1_categorisation_plotting) %>%
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    confidence_in_approach = as.factor(confidence_in_approach)
    ) %>% 
  calculate_conclusion(., confidence_in_approach, categorisation)
```

```{r echo=FALSE, message=FALSE}
conclusions_suitability_plot <-
  plot_conclusion(
    conclusions_suitability_data,
    confidence_in_approach,
    categorisation,
    with_labels = TRUE,
    y_lab = "Suitability of the self-judged analysis"
  ) +
  theme(
    legend.text = element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal"
  )

ggsave(here::here("figures/conclusions_suitability_plot.jpg"), conclusions_suitability_plot, dpi = 300)

conclusions_suitability_plot
```

#### Robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following figure shows that for that...,

```{r include=FALSE}
processed %>% 
  ggplot() + 
  # Converting to natlog because of extreme outlier
  aes(x = log(reanalysis_model_sample_size)) + 
  geom_histogram()

processed %>% 
  count(is.na(reanalysis_model_sample_size))
```

```{r echo=FALSE, message=FALSE}
conclusions_samplesize_plot <-
  processed %>%
  rename(categorisation = task1_categorisation_plotting) %>%
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_model_sample_size) %>%
  # TODO: There are missing sample size values what to do with them?
  filter(!is.na(reanalysis_model_sample_size)) %>%
  mutate(
    categorisation = fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    ),
    log_model_sample_size = log(reanalysis_model_sample_size)
  ) %>%
  # filter(reanalysis_model_sample_size != max(reanalysis_model_sample_size)) %>%
  ggplot() +
  aes(x = categorisation, y = log_model_sample_size) +
  geom_rain(rain.side = 'l') +
  labs(y = "Log sample size") +
  theme(
    axis.title.x = element_blank(),
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line()
  )

ggsave(here::here("figures/conclusions_samplesize_plot.jpg"), conclusions_samplesize_plot, dpi = 300)

conclusions_samplesize_plot
```

#### How robust are statistical findings published in social sciences to analytical choices?

A main question of our study was whether different analysts arrive at the same effect estimates (+/- 0.05 Cohen's d) as the analyst of the original study?

```{r include=FALSE}
# Number of cases where the original effect size is missing
missing_original_n <-
  processed %>% 
  distinct(paper_id, .keep_all = T) %>% 
  count(is.na(original_cohens_d)) %>% 
  filter(`is.na(original_cohens_d)`) %>% 
  pull(n)

# Check if there are cases where there is missing original effect size on the paper level but not for every analysis
# Would mean that there is a mistake in the merging code
processed %>% 
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) %>% 
  group_by(paper_id) %>% 
  mutate(
    number_of_analysis = n() 
  ) %>% 
  filter(is.na(original_cohens_d)) %>% 
  mutate(
    number_of_analysis_after_filtering = n()
  ) %>% 
  ungroup() %>% 
  mutate(
    match = if_else(number_of_analysis == number_of_analysis_after_filtering, TRUE, FALSE)
  )

# Number of cases where the reanalysed effect size is missing
missing_reanalysis_n <-
  processed %>% 
  count(is.na(reanalysis_cohens_d)) %>% 
  filter(`is.na(reanalysis_cohens_d)`) %>% 
  pull(n)

# Are there any cases where the original effect size is present but the reanalyzed is not and vica versa?
processed %>% 
  dplyr::select(original_cohens_d, reanalysis_cohens_d) %>% 
  mutate(
    both_present_or_missing = is.na(original_cohens_d) == is.na(reanalysis_cohens_d)
  ) %>% 
  count(both_present_or_missing)

# Check if there are any cases where everything is missing
processed %>% 
  group_by(paper_id) %>%
  mutate(
    both_missing = all(is.na(original_cohens_d) & is.na(reanalysis_cohens_d))
  ) %>%
  ungroup() %>% 
  count(both_missing)
  
# Preparing the data for the plot
# TODO: use separate datasets to plot
reanalysis_data <-
  processed %>% 
  dplyr::select(simplified_paper_id, analyst_id, reanalysis_cohens_d, original_cohens_d) %>% 
  group_by(simplified_paper_id) %>% 
  rename(effect_size = reanalysis_cohens_d) %>% 
  mutate(
    effect_size_type = paste0("re-analysis_0", row_number()),
    ) %>% 
  ungroup() %>% 
  mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = fct_reorder(simplified_paper_id,
                                      original_cohens_d,
                                      .na_rm = FALSE)
  )

original_data <-
  processed %>% 
  distinct(simplified_paper_id, original_cohens_d) %>%
    mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = fct_reorder(simplified_paper_id,
                                      original_cohens_d,
                                      .na_rm = FALSE)
  )
```

```{r echo=FALSE, warning=FALSE}
# Colors
color_vector <- setNames(c("#F8766D", "#CD9600", "#7CAE00", "#00BE67", "#00BFC4", "#00A9FF", "#C77CFF"), paste0("re-analysis_", sprintf("%02d", 1:7)))

effect_main_plot <-
  reanalysis_data %>% 
  # TODO: what to do with huge cohens ds?
  filter(effect_size <= 10 & effect_size >= -10) %>% 
  ggplot() +
  aes(
    y = simplified_paper_id,
    x = effect_size,
    color = effect_size_type
  ) +
  geom_point(shape = 16) +
  scale_color_manual(values = color_vector) +
  geom_pointrange(
    data = original_data,
    aes(
      x = original_cohens_d,
      xmin = tolarence_region_lower,
      xmax = tolarence_region_upper,
      alpha = 0.8
      ),
    show.legend = FALSE,
    color = "black",
    shape = 15,
    ) +
  labs(
    x = "Effect size in Cohen's d"
  ) +
  guides(color = "none") +
  theme(
    axis.ticks = element_blank(),
    legend.title = element_blank(),
    axis.line = element_line(),
    axis.title.y = element_blank(),
    plot.margin = margin(t = 10, r = 20, b = 10, l = 10, "pt"),
    panel.grid = element_line(color = "lightgray"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.background = element_blank(),
    # axis.text.y=element_text(margin = margin(1, unit = "cm"), vjust =1.5)
    )

# TODO: for some reason tolerance region is not showing on saved plot... why?
ggsave(here::here("figures/effect_main_plot.jpg"), plot = effect_main_plot,
       width = 13, height = 11.69,
       dpi = 300)

effect_main_plot
```

Fig X displays the calculated Cohen's d-s for the re-analyses of each study and the Cohen's d for the original study with a 0.05 tolerance region around it. For `r missing_original_n` papers the Cohen's d for the original analysis cannot be calculated. In these cases, we could not calculate a tolerance region for the given paper, however, we show the available effect sizes for the re-analyses. For `r missing_reanalysis_n` analysis by the re-analyst the Cohen's d cannot be calculated.

```{r echo=FALSE}
# TODO: merge marginal effect sizes and create the plot
```

Fig X displays the calculated marginal ES-s for the re-analyses of each study (where they are available)

```{r echo=FALSE}
# TODO: merge marginal effect sizes and create the plot
```

Fig X displays the calculated marginal ES-s AND corresponding Cohen's d-s for the re-analyses of each study (where they are available)

```{r echo=FALSE}
effect_region_all_data <- calculate_tolerance_region(data = processed, grouping_var = simplified_paper_id, drop_missing = T)
  
effect_region_all_plot <- plot_tolarence_region(effect_region_all_data, simplified_paper_id)

ggsave(here::here("figures/effect_region_all_plot.jpg"), effect_region_all_plot, width = 8.27, height = 11.69, dpi = 300)

effect_region_all_plot
```

```{r include=FALSE}
within_tolerance_region <-
  effect_region_all_data %>% 
  group_by(simplified_paper_id) %>% 
  summarise(
    robust = if_else(any(is_within_region == "Yes" & relative_frequency == 1), "Robust", "Not Robust")
    ) %>% 
  ungroup() %>% 
  count(robust) %>% 
  mutate(
    N = sum(n),
    relative_frequency = n / N,
    percentage = round(relative_frequency * 100)
    )
```

Here, we were interested what percentage of the new effect sizes were beyond the tolerance region (+/- 0.05 Cohen's d). We found that `r filter(within_tolerance_region, robust == "Not robust") %>% pull(percentage)`% (`r filter(within_tolerance_region, robust == "Not robust") %>% pull(n)` out of `r filter(within_tolerance_region, robust == "Not robust") %>% pull(N)`) of the studies contained at least one re-analysis result where the effect size was beyond the tolerance region (+/- 0.05 Cohen's d) of the result of the original study.

##### Robustness by field

We were interested to see whether these results show a different pattern when inspecting them in different fields. The following figure shows that for the major fields (\>=10 studies) the pattern were...,

```{r include=FALSE, warning=FALSE}
processed %>% 
  distinct(paper_id, .keep_all = T) %>% 
  count(paper_discipline) %>% 
  arrange(n)
```

```{r echo=FALSE}
effect_region_discipline_data <- 
  processed %>% 
  filter(paper_discipline %in% c("psychology", "economics", "political science")) %>%
  mutate(paper_discipline = str_to_title(paper_discipline)) %>% 
  calculate_tolerance_region(., paper_discipline, drop_missing = TRUE)
# Missing values were excluded

effect_region_discipline_plot <- plot_tolarence_region(effect_region_discipline_data, paper_discipline, with_labels = TRUE)

ggsave(here::here("figures/effect_region_discipline_plot.jpg"), effect_region_discipline_plot, dpi = 300)

effect_region_discipline_plot
```

```{r echo=FALSE}
effect_robustness_discipline_data <-
  processed %>% 
  filter(paper_discipline %in% c("psychology", "economics", "political science")) %>% 
  mutate(paper_discipline = str_to_title(paper_discipline)) %>% 
  calculate_robustness(paper_discipline)

effect_robustness_discipline_plot <- plot_robustness(effect_robustness_discipline_data, paper_discipline)

ggsave(here::here("figures/effect_robustness_discipline_plot.jpg"), effect_robustness_discipline_plot, dpi = 300)

effect_robustness_discipline_plot 
```

```{r echo=FALSE}
effect_robustness_discipline_rain_plot <-
  effect_robustness_discipline_data %>%
  ggplot() +
  aes(x = paper_discipline, y = robustness) +
  geom_rain(rain.side = 'l') +
  # geom_point(aes(x = paper_discipline, y = cohen_original), color = "red") +
  labs(y = "Robustness") +
  theme(
    axis.title.x = element_blank(),
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line()
  )

ggsave(here::here("figures/effect_robustness_discipline_rain_plot.jpg"), effect_robustness_discipline_rain_plot, dpi = 300)

effect_robustness_discipline_rain_plot
```

##### Robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type. The following figure shows that for that...,

```{r include=FALSE}
processed %>% 
  distinct(paper_id, .keep_all = T) %>% 
  count(experimental_or_observational) %>% 
  arrange(n)
```

```{r echo=FALSE, message=FALSE}
effect_region_studytype_data <- 
  processed %>% 
  mutate(experimental_or_observational = str_to_title(experimental_or_observational)) %>% 
  calculate_tolerance_region(., experimental_or_observational, drop_missing = TRUE)

effect_region_studytype_plot <-
  plot_tolarence_region(
    effect_region_studytype_data,
    experimental_or_observational,
    with_labels = TRUE
  )

ggsave(here::here("figures/effect_region_studytype_plot.jpg"), effect_region_studytype_plot, dpi = 300)

effect_region_studytype_plot
```

```{r echo=FALSE, message=FALSE}
effect_robustness_studytype_data <-
  processed %>% 
  mutate(experimental_or_observational = str_to_title(experimental_or_observational)) %>% 
  calculate_robustness(experimental_or_observational)

effect_robustness_studytype_plot <- plot_robustness(effect_robustness_studytype_data, experimental_or_observational, xlab = "Study type")

ggsave(here::here("figures/effect_robustness_studytype_plot.jpg"), effect_robustness_studytype_plot, dpi = 300)

effect_robustness_studytype_plot 
```

##### Robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts. The following figure shows that for that...,

```{r include=FALSE}
processed %>% 
  distinct(expertise_self_rating)
```

```{r echo=FALSE, message=FALSE}
effect_region_expertise_data <- 
  processed %>% 
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(expertise_self_rating, levels = c(
      "1\n(Beginner)",
      as.character(2:9),
      "10\n(Expert)")
    )
  ) %>% 
  calculate_tolerance_region(., expertise_self_rating, drop_missing = TRUE)

effect_region_expertise_plot <- plot_tolarence_region(effect_region_expertise_data, expertise_self_rating)

ggsave(here::here("figures/effect_region_expertise_plot.jpg"), effect_region_expertise_plot, dpi = 300)

effect_region_expertise_plot
```

##### Robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-effect-region-familiarity shows that for that...,

```{r echo=FALSE, message=FALSE, message=FALSE}
#| label: fig-effect-region-familiarity
#| fig-cap: "The figure shows..."

effect_region_familiarity_data <- calculate_tolerance_region(processed, familiar_with_paper, drop_missing = TRUE)

effect_region_familiarity_plot <- plot_tolarence_region(effect_region_familiarity_data, familiar_with_paper, ylab = "Familiar with the paper")

ggsave(here::here("figures/effect_region_familiarity_plot.jpg"), effect_region_familiarity_plot, dpi = 300)

effect_region_familiarity_plot
```

##### Robustness by the suitability of their self-judged analysis

```{r include=FALSE}
processed %>% 
  distinct(confidence_in_approach)
```

```{r echo=FALSE, message=FALSE}
effect_region_suitability_data <- calculate_tolerance_region(processed, confidence_in_approach, drop_missing = TRUE)

effect_region_suitability_plot <- plot_tolarence_region(effect_region_suitability_data, confidence_in_approach, ylab = "Suitability of their\nself-judged analysis")

ggsave(here::here("figures/effect_region_suitability_plot.jpg"), effect_region_suitability_plot, dpi = 300)

effect_region_suitability_plot
```

##### Robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following @fig-samplesize-region shows that for that..., 

```{r echo=FALSE, message=FALSE}
#| label: fig-samplesize-region
#| fig-cap: "The figure shows how the sample size influences whether the re-analysis effect sizes were within the tolerance region. For the figure we did not inlcude: those studies were the original effect sizes were missing, and cases where the re-analysis effect size or sample size were missing."

samplesize_region_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_model_sample_size,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = case_when(
      reanalysis_cohens_d <= tolarence_region_lower |
        reanalysis_cohens_d >= tolarence_region_upper ~ "No",
      reanalysis_cohens_d >= tolarence_region_lower |
        reanalysis_cohens_d <= tolarence_region_upper ~ "Yes",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = as.factor(is_within_region),
    log_model_sample_size = log(reanalysis_model_sample_size)
  ) |> 
  filter(
    !is.na(original_cohens_d),
    !is.na(log_model_sample_size),
    is_within_region != "Missing")

samplesize_region_plot <-
  samplesize_region_data |> 
  ggplot() +
  aes(x = is_within_region, y = log_model_sample_size) +
	geom_rain(rain.side = 'l') +
  labs(
    y = "Log sample size",
    x = "Is the re-analysis effect size within the tolerance region?"
  ) +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line()
  )

ggsave(here::here("figures/effect_region_samplesize_plot.jpg"), samplesize_region_plot, dpi = 300)

samplesize_region_plot
```

