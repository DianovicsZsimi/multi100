---
title: "Results"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)

# Load custom functions
source(here::here("R/utils.R"))

# Read processed data files
processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_not_reviewed <- readr::read_csv(here::here("data/processed/multi100_peer-eval-not-reviewed_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

## Transform datafiles for analysis 
# Add number of evaluations per analysis
peer_eval <-
  peer_eval |>
  dplyr::group_by(paper_id, analyst_id) |>
  dplyr::mutate(n_peer_evals = dplyr::n()) |>
  dplyr::ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

# Transforming the timestamp to date type from character
processed <-
  processed |>
  dplyr::mutate(
    task1_timestamp = lubridate::ymd_hms(task1_timestamp))
```

```{r include=FALSE}
# Check if the analyst_id's are always unique to one person
processed |>
  dplyr::distinct(first_name, last_name, analyst_id) |>
  dplyr::group_by(first_name, last_name) |>
  dplyr::mutate(n_analyst_id = dplyr::n()) |>
  dplyr::arrange(dplyr::desc(n_analyst_id))

# Check if there are peer evaluators who evaluated an analysis for the same paper they analyzed
peer_eval |> 
  dplyr::select(evaluator_id, paper_id) |> 
  rename(evaluator_paper_id = paper_id) |>
  left_join(distinct(processed, paper_id, analyst_id), by = c("evaluator_id" = "analyst_id"), relationship = "many-to-many") |> 
  mutate(match = case_when(
    paper_id == evaluator_paper_id ~ TRUE,
    paper_id != evaluator_paper_id ~ FALSE
  )) |> 
  filter(match)
```

## General descriptives

```{r include=FALSE}
analyst_signed_up <-
  all_people |>
  dplyr::mutate(first_name = tolower(first_name),
                last_name = tolower(last_name)) |>
  dplyr::distinct(first_name, last_name, .keep_all = T) |>
  dplyr::filter(disclosure_agreement == "I agree")

analyst_submitted <-
  processed |>
  dplyr::distinct(analyst_id) |>
  nrow()
```

As a response to our recruitment call, `r nrow(analyst_signed_up)` researchers signed up to participate in our study. Out of these volunteers, `r analyst_submitted` signed up to analyse at least one dataset and submitted their work by the deadline or an extended deadline.

```{r include=FALSE}
n_analysis <-
  processed |> 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of co-analysts as some co-analysts volunteered to analyse more than one dataset.

```{r include=FALSE}
n_failed_peer <-
  processed |>
  dplyr::filter(!peer_eval_pass | !incomplete_response_pass) |>
  nrow()
```

Out of the submitted analyses `r nrow(dplyr::filter(processed, !peer_eval_pass))` of them were omitted from the summary analysis as their analysis failed the peer evaluation and an additional `r nrow(dplyr::filter(processed, !incomplete_response_pass))` were excluded because of incomplete responses.

```{r include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  dplyr::filter(peer_eval_pass & incomplete_response_pass)
```

```{r include=FALSE}
final_n_analyst <-
  processed |> 
  dplyr::distinct(analyst_id) |> 
  nrow()
```

As a result, we ended up with `r nrow(processed)` re-analyses, submitted by `r final_n_analyst` co-analysts.

Although we invited more than 5 co-analysts to each of the 100 studies, due to drop-outs and peer evaluation exclusions the final number of completed analyses ranged between `r min(dplyr::count(processed, paper_id)$n)` and `r max(dplyr::count(processed, paper_id)$n)`. Table 1 shows the the distribution of analyses per studies. 

```{r echo=FALSE, message=FALSE}
processed |>
  dplyr::count(paper_id) |> 
  dplyr::count(n, name = "N") |>
  dplyr::rename(`Number of Completed Analyses` = n,
                `Number of Studies` = N) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

## Basic demographics of the co-analysts

```{r include=FALSE}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position_grouped)

position <-
  processed |>
  dplyr::select(analyst_id,
                paper_id,
                current_position_grouped,
                task1_timestamp) |>
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) |>
  count(current_position_grouped) |>
  rename(position = current_position_grouped) |>
  mutate(position = factor(
    position,
    levels = c(
      "Professor",
      "Associate Professor",
      "Assistant Professor",
      "Post-Doc Researcher",
      "Doctoral Student",
      "Other academic/research position"
    )
  ))
```

Out of all the co-analysts who submitted their work by the deadline, there were `r dplyr::filter(position, position == "Professor") |> dplyr::pull(n)` professors, `r dplyr::filter(position, position == "Associate Professor") |> dplyr::pull(n)` associate professors, `r filter(position, position == "Assistant Professor") |> dplyr::pull(n)` assistant professors, `r dplyr::filter(position, position == "Post-Doc Researcher") |> dplyr::pull(n)` post-doctoral researchers, `r dplyr::filter(position, position == "Doctoral Student") |> dplyr::pull(n)` doctoral students, `r dplyr::filter(position, position == "Other academic/research position") |> dplyr::pull(n)` from other academic/research positions. 

```{r include=FALSE}
check_diff_response(processed, analyst_id, gender)

gender <-
  processed |>
  dplyr::select(analyst_id, paper_id, gender, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(gender)
```

The gender distribution of the co-analysts is as follows: `r dplyr::filter(gender, gender == "Female") |> dplyr::pull(n)` female, `r dplyr::filter(gender, gender == "Male") |> dplyr::pull(n)` male, `r dplyr::filter(gender, gender == "Non-binary") |> dplyr::pull(n)` other, and `r dplyr::filter(gender, gender == "Prefer not to say") |> dplyr::pull(n)` didn't want to respond to this question.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = age)

age <-
  processed |>
  dplyr::select(analyst_id, paper_id, age, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Filter erroneous data
  dplyr::filter(age != "00") |> 
  dplyr::mutate(age = as.numeric(age))
  
age_group <-
  age |> 
  dplyr::mutate(
    age_group = dplyr::case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    ),
    age_group = factor(age_group, levels = c("young", "middle", "old"))
    ) |> 
  dplyr::count(age_group) |> 
  tidyr::complete(age_group, fill = list(n = 0))
```

The age distribution of the co-analysts is depicted in @fig-age-plot. `r dplyr::filter(age_group, age_group == "young") |> dplyr::pull(n)` young adults (-39 years); `r dplyr::filter(age_group, age_group == "middle") |> dplyr::pull(n)` middle-aged adults (40-59 years); and no old adults (60- years).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-age-plot
#| fig-cap: "The figure shows the distribution of the analysts' age. When an analyst submitted more than one re-analysis with more than a year apart, we only kept their age at the time of their first submission. Moreover, one analyst was excluded because they did not disclose their age."
age_plot <- 
  age |>
  ggplot2::ggplot() +
  ggplot2::aes(x = age) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::scale_y_continuous(expand = c(0, 0), limits = c(0, 40)) +
  ggplot2::scale_x_continuous(
    # limits = c(20, 55),
    # breaks = c(20, 30, 40, 50, 60),
    # labels = c("20", "30", "40", "50", "60")
  ) +
  ggplot2::labs(x = "Age (years)",
       y = "Number of co-analysts") +
  ggplot2::theme(
    panel.grid = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.line = ggplot2::element_line()
  )

ggplot2::ggsave(here::here("figures/demographic_age_plot.jpg"), age_plot, dpi = 300)

age_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = education_level)

education <-
  processed |>
  dplyr::select(analyst_id, paper_id, education_level, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(education_level) |> 
  dplyr::rename(education = education_level)
```

Regarding the highest level of education, `r dplyr::filter(education, education == "High-school diploma or equivalent") |> dplyr::pull(n)` reported High-school diploma or equivalent, `r dplyr::filter(education, education == "Bachelor's degree or equivalent") |> dplyr::pull(n)` co-analysts had Bachelor's degree or equivalent, `r dplyr::filter(education, education == "Master's degree or equivalent") |> dplyr::pull(n)` Master's degree or equivalent, `r dplyr::filter(education, education == "Doctoral degree or equivalent") |> dplyr::pull(n)` had Doctoral degree or equivalent. In case the analysts completed more than one re-analysis and they advanced in their studies by the time of their second analysis, we only kept their first response for this comparison.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = country_of_residence)

country_data <- raster::ccodes()

country <- 
  processed |>
  dplyr::select(analyst_id, paper_id, country_of_residence, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(country_of_residence) |> 
  dplyr::rename(region = country_of_residence) |> 
  # Modify country names to fit the worldmap data
  dplyr::mutate(
    subregion = dplyr::case_when(
      region == "Hong Kong (China)" ~ "Hong Kong",
      TRUE ~ NA_character_
    ),
    region = dplyr::case_when(
      region == "Hong Kong (China)" ~ "China",
      region == "United States" ~ "USA",
      region == "United Kingdom" ~ "UK",
      TRUE ~ region
    ),
    continent = countrycode::countrycode(region, "country.name", "continent"),
    iso3_code = countrycode::countrycode(region, "country.name", "iso3c")
  ) |> 
  dplyr::left_join(dplyr::select(country_data, ISO3, UNREGION1), by = c("iso3_code" = "ISO3"))

continent <-
  country |> 
  dplyr::group_by(continent) |> 
  dplyr::summarise(N = sum(n))

region <-
  country |> 
  dplyr::group_by(UNREGION1) |> 
  dplyr::summarise(N = sum(n))
```

The country of residence of the co-analysts is shown on the map on @fig-country-plot. Regarding the continents, `r dplyr::filter(continent, continent == "Africa") |> dplyr::pull(N)` co-analyst was from Africa, `r dplyr::filter(continent, continent == "Asia") |> dplyr::pull(N)` were from Asia, `r dplyr::filter(continent, continent == "Oceania") |> dplyr::pull(N)` from Oceania, `r dplyr::filter(continent, continent == "Europe") |> dplyr::pull(N)` from Europe, `r dplyr::filter(region, UNREGION1 == "Northern America") |> dplyr::pull(N)` from North America, `r dplyr::filter(region, UNREGION1 %in% c("Central America", "South America")) |> dplyr::summarise(sum(N)) |> dplyr::pull("sum(N)")` from South America.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-country-plot
#| fig-cap: "The figure shows the analysts' country of residence. When an analyst submitted more than one re-analysis and they moved between the submissions, we only kept their first response."

world_map <- 
  ggplot2::map_data("world") |> 
  dplyr::mutate(
    subregion = dplyr::case_when(
      subregion == "Hong Kong" ~ subregion,
      TRUE ~ NA_character_
    )
  )

country_map <- dplyr::left_join(world_map, country, by = c("region", "subregion"))

country_map_plot <- 
  country_map |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = long, y = lat, group = group, fill = n) +
  ggplot2::geom_polygon(color = "white", linewidth = 0.2) +
  ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Number of\nanalyst") +
  ggplot2::theme_void()

ggplot2::ggsave(here::here("figures/demographic_country_plot.jpg"), country_map_plot, dpi = 300)

country_map_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = primary_discipline)

analyst_discipline <- 
  processed |>
  dplyr::select(analyst_id, paper_id, primary_discipline, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  calculate_percentage(response_var = primary_discipline) |>
  dplyr::rename(discipline = primary_discipline) |> 
  dplyr::arrange(dplyr::desc(dplyr::if_else(discipline == "Other", -Inf, percentage)))
```

We asked the co-analysts which discipline is the closest to their research area. The following Table summarizes the distribution of their disciplinary orientation. Co-analysts from `r dplyr::slice(analyst_discipline, 1) |> dplyr::pull(discipline)` and `r dplyr::slice(analyst_discipline, 2) |> dplyr::pull(discipline)` disciplines participated in the highest ratio in this study.

```{r, echo=FALSE}
# tbl-discipline
# "Distribution of the Analysts' Primary Discipline"

analyst_discipline |>
  dplyr::select(discipline, n, percentage) |>
  dplyr::rename(Discipline = discipline,
                Count = n,
                Percentage = percentage) |>
  gt::gt() |>
  tab_style(style = gt::cell_text(weight = "bold"),
            locations = gt::cells_column_labels()) |>
  gt::tab_footnote(
    "Note: Whenever the respondents provided more than one field we only kept their first responses."
  )
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = years_of_experience)

analyst_experience_years_data <-
  processed |>
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Dropped because of faulty response
  dplyr::filter(analyst_id != "RTX71")
```

The distribution of the years of experience with data analysis is depicted on @fig-experience-years-plot. The median time of experience with data analysis was `r median(analyst_experience_years_data$years_of_experience)` years among our co-analysts.

```{r echo=FALSE, message=FALSE}
#| label: fig-experience-years-plot
#| fig-cap: "The figure shows the analysts' years of experience with data analysis. When an analyst submitted more than one re-analysis and a year passed between the responses we only kept their first response."
analyst_experience_years_plot <-
  analyst_experience_years_data |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = years_of_experience) +
  ggplot2::geom_histogram() +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Years of experience with data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/demographic_experience_years_plot.jpg"), analyst_experience_years_plot, dpi = 300)

analyst_experience_years_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = analysis_frequency)

analysis_frequency_count <-
  processed |> 
  dplyr::select(analyst_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(analysis_frequency)
```

We asked our co-analysts how regularly they perform data analysis. @fig-analysis-frequency shows that the most frequent category was `r dplyr::filter(analysis_frequency_count, n == max(n)) |> dplyr::pull(analysis_frequency)`.

```{r echo=FALSE, message=FALSE}
#| label: fig-analysis-frequency
#| fig-cap: "The figure shows how regularly the analysts perform data analysis."

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count |>
  dplyr::mutate(
    analysis_frequency = dplyr::case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = forcats::fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot2::ggplot() +
  ggplot2::aes(x = analysis_frequency, y = n) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Frequency of doing data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/demographic_analysis_frequency_plot.jpg"), analysis_frequency_plot, dpi = 300)

analysis_frequency_plot
```

```{r include=FALSE}
expertise_self_rating_data <-
  processed |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |>
  dplyr::count(expertise_self_rating)
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution on @fig-self-rating-plot shows that the most prevalent answer was `r dplyr::filter(expertise_self_rating_data, n == max(n)) |> dplyr::pull(expertise_self_rating)` .

```{r echo=FALSE, message=FALSE}
#| label: fig-self-rating-plot
#| fig-cap: "The figure shows the analysts' self-rated level of expertise in data analysis. When an analyst submitted more than one re-analysis we only kept their first response."

expertise_self_rating_plot <-
  expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(
      expertise_self_rating,
      c("1\n(Beginner)",
        as.character(2:9),
        "10\n(Expert)")
    )
  ) |>
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-rated expertise of data analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_expertise_self_rating_plot.jpg"), expertise_self_rating_plot, dpi = 300)

expertise_self_rating_plot
```

```{r include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

In `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(N)`) of the cases, the co-analysts were familiar with the paper that the provided dataset belongs to before beginning their work on the project.

```{r include=FALSE}
processed |> 
  count(communication_check)
```

All co-analyst reported that they have not communicated about the details of their analysis with other co-analysts working with the same dataset.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n)) |> 
  mutate(
    software = case_when(
      software %in% c("r", "stata", "spss", "jasp") ~ toupper(software),
      TRUE ~ stringr::str_to_title(software)
    )
  )
```

We asked the co-analysts what programming language/software/tool they used in their data analysis during Task 1 and Task 2. The following figure indicates that `r slice(software_data, 1) |> pull(software)` (`r slice(software_data, 1) |> pull(percentage)`%), `r slice(software_data, 2) |> pull(software)` (`r slice(software_data, 2) |> pull(percentage)`%), and `r slice(software_data, 3) |> pull(software)` (`r slice(software_data, 3) |> pull(percentage)`%) were the most popular responses. @fig-software shows the distribution of all the responses.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-software
#| fig-cap: "The figure shows which software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported the use of multiple software we kept all their responses for this figure. The figure shows only software that was used by more than 1% of the analysts."

software_plot <-
  software_data |>
  dplyr::filter(percentage > 1) |>
  ggplot() +
  aes(x = reorder(software, -percentage),
      y = percentage) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0),
                     labels = scales::percent_format(scale = 1)) +
  labs(y = "Percentage of co-analysts",
       x = "Software") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_software_plot.jpg"), software_plot, dpi = 300)

software_plot
```

## Descriptives of the statistical analyses

A difference in Task 2 compared to Task 1 was that the co-analysts received some constraints for their analysis to make their result comparable to a single result in the original study (see Methods for more details).

```{r include=FALSE}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the co-analysts to present one main statistical result, in `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(percentage)`% of the analyses (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(N)`), conclusion was based on the p-value. Bayes Factor was used in `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(percentage)`% of the cases (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(N)`).

```{r include=FALSE}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

For `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(percentage)` % (`r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(N)`) of the analyses, the co-analysts reported having to make additional calculations in Task 2 compared to Task 1. In the remaining `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(N)`) of the cases, the co-analysts indicated that despite the requirements of the instructions, they could conduct the same analyses as in Task 1.

```{r include=FALSE}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(n)` out of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r include=FALSE}
total_hours_data <-
  processed |> 
  filter(total_hours != 999)
```

The co-analysts were asked to estimate the time they spent performing Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (@fig-total-hours).

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-total-hours
#| fig-cap: "The figure shows the total hours the analyst spent on Task 1 and Task 2 together. In case an analyst completed multiple re-analyses, we kept all their responses for this figure. One response was excluded due to being an outlier (999 hours)."

total_hours_plot <-
  total_hours_data |>
  ggplot() +
  aes(x = total_hours) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Total hours spent on the analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_total_hours_plot.jpg"), total_hours_plot, dpi = 300)

total_hours_plot
```

## Peer evaluation
### Peer evaluators
#### Basic demographics of the peer evaluators 

```{r include=FALSE}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval |> 
  distinct(evaluator_id) |> 
  inner_join(processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data |> 
  count(evaluator_id) |> 
  arrange(desc(n))

# Check if a peer evaluator has more than one evaluation submitted
peer_eval |>
  count(evaluator_id) |>
  arrange(desc(n))
```

@fig-evaluator-years shows that most peer evaluators have many years of experience with conducting statistical analysis. 

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-years
#| fig-cap: "The figure shows the peer evaluators’ years of experience with data analysis. When a peer evaluator submitted more than one evaluation and a year passed between the responses, we kept only their first response."

peer_analyst_experience_years_data <-
  peer_evaluator_data |>
  dplyr::select(evaluator_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data |>
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_experience_years_plot.jpg"), peer_analyst_experience_years_plot, dpi = 300)

peer_analyst_experience_years_plot
```

@fig-evaluator-analysis-frequency indicates that peer evaluators regularly perform data analysis. 

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-analysis-frequency
#| fig-cap: "The figure shows how regularly the peer evaluators perform data analysis."

peer_analysis_frequency_count <-
  peer_evaluator_data |> 
  dplyr::select(evaluator_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |> 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <-
  peer_analysis_frequency_count |>
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of doing data analysis",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_analysis_frequency_plot.jpg"), peer_analysis_frequency_plot, dpi = 300)

peer_analysis_frequency_plot
```

@fig-evaluator-expertise indicates that most peer evaluators rate themselves close to expert level in data analysis. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-expertise
#| fig-cap: "The figure shows the peer evaluators’ self-rated level of expertise in data analysis. When a peer evaluator submitted more than one re-analysis, we kept only their first response."

peer_expertise_self_rating_data <-
  peer_evaluator_data |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |>
  count(expertise_self_rating) |> 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) |>
  tidyr::complete(expertise_self_rating, fill = list(n = 0)) |> 
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-reported expertise rating",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_expertise_self_rating_plot.jpg"), peer_expertise_self_rating_plot, dpi = 300)

peer_expertise_self_rating_plot
```

### Peer evaluations
#### Descriptives of peer evaluations 

```{r include=FALSE}
# Number of peer evaluations
# One response was excluded in multi100_raw_processed because the evaluator did not provide the analyst_id
nrow(peer_eval_not_reviewed)
nrow(peer_eval)
```

In total, we received `r nrow(peer_eval_not_reviewed) + 1` peer evaluation reports. One peer evaluation was removed because the ID of the analyst was not provided, and as such, we could not verify with certainty which re-analysis was being evaluated leaving us with a total of `r nrow(peer_eval_not_reviewed)` peer evaluation reports on `r nrow(distinct(peer_eval_not_reviewed, paper_id))` different papers. After the panel member review of the peer evaluations (see Peer Evaluation: Review and Decisions’ supplement for all decisions and reasoning behind each case), the final result of the peer evaluation was the following.  

```{r include=FALSE}
dplyr::count(peer_eval, task1_pipeline_acceptable)
dplyr::count(peer_eval, task1_conclusion_follows_results)
dplyr::count(peer_eval, task2_pipeline_acceptable)
```

At the end of the peer evaluation process, one analysis was deemed to contain an unacceptable analysis pipeline. Therefore, we removed this single analysis from our results. For the remaining analyses, it was determined that all task 1 and task 2 analysis pipelines were acceptable. Furthermore, all remaining task 1 conclusions were considered to accurately follow on from the results, and the analysts self-categorization of the results were considered adequate.

```{r include=FALSE}
reproducibility_checks_data <-
  peer_eval |> 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_n <-
  reproducibility_checks_data |> 
  filter(any_code_mismatches %in% c("(3) I executed it and I found no mismatches", "(4) I executed it and I found mismatches")) |> 
  summarise(n_reproducibility_checks = sum(n))
```

`r reproducibility_checks_n` analytical reproducibility checks was successfully conducted which identified mismatches in `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)` analyses.  

#### Peer evaluation procedure results
##### Main outputs of the peer evaluation

```{r include=FALSE}
count(peer_eval_not_reviewed, task1_pipeline_acceptable)
count(peer_eval_not_reviewed, task1_conclusion_follows_results)
count(peer_eval_not_reviewed, task1_categorisation_is_accurate)
count(peer_eval_not_reviewed, task2_pipeline_acceptable)
```

Accordingly, the task 1 analysis pipeline was rated as ‘Unacceptable’ in `r nrow(filter(peer_eval_not_reviewed, task1_pipeline_acceptable == "(1) Unacceptable"))` cases, the task 1 conclusion was judged not to follow adequately from the results in `r nrow(filter(peer_eval_not_reviewed, task1_conclusion_follows_results == "(2) It does not follow adequately from the results of the analysis"))` cases, the task 1 self-categorization of the result was rated as ‘inadequate’ in `r nrow(filter(peer_eval_not_reviewed, task1_categorisation_is_accurate == "(2) Inadequate"))` cases, the task 2 analysis pipeline was rated as ‘unacceptable’ in `r nrow(filter(peer_eval_not_reviewed, task2_pipeline_acceptable == "(1) Unacceptable"))` cases while the task 2 analysis pipeline was judged as ‘incomplete or missing’ in `r nrow(filter(peer_eval_not_reviewed, task2_pipeline_acceptable == "(5) Incomplete or missing analysis"))` cases, while the code reproducibility checks revealed `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)` mismatches.

```{r include=FALSE}
peer_eval_subset_task1_data <- 
  peer_eval |>  
  dplyr::filter(n_peer_evals > 1) |>  
  dplyr::select(paper_id, analyst_id, task1_pipeline_acceptable) |>  
  dplyr::group_by(paper_id, analyst_id) |>  
  dplyr::summarise(
    common_task1_acceptable = ifelse(dplyr::n_distinct(task1_pipeline_acceptable) == 1, first(task1_pipeline_acceptable), NA),
    .groups = 'drop'
  ) |> 
  dplyr::filter(!is.na(common_task1_acceptable)) |> 
  dplyr::filter(common_task1_acceptable != "(2) Acceptable but low quality") |> 
  left_join(dplyr::select(processed, paper_id, analyst_id, task1_categorisation_plotting), by = c("paper_id", "analyst_id")) |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = common_task1_acceptable,
                       categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-subset-task1-pipeline
#| fig-cap: "The figure shows the inferential robustness of the studies by the acceptability of the analysis pipelines according to the peer evaluators. For this figure we only included studies with more than one peer evaluation and where the peer evaluators agreed on their rating. The figure only shows the studies with a medium and high quality of analysis pipelines."

peer_eval_subset_task1_plot <-
  plot_percentage(
    data = peer_eval_subset_task1_data,
    categorization_var = categorisation,
    grouping_var = common_task1_acceptable,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "Peer evaluation rating of\nTask 1 analysis pipeline"
  )

ggsave(here::here("figures/peer_eval_subset_task1_plot.jpg"), peer_eval_subset_task1_plot, dpi = 300)

peer_eval_subset_task1_plot
```

```{r include=FALSE}
peer_eval |> 
  dplyr::select(paper_id, analyst_id, evaluator_id, task1_pipeline_acceptable) |>
  count(paper_id, analyst_id) |> 
  arrange(n)

calculate_percentage(peer_eval, task1_pipeline_acceptable) |>
  dplyr::select(
    `Task 1 analysis pipeline evaluation rating` = task1_pipeline_acceptable,
    `Number of occurances` = n,
    `Number of all responses` = N,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

##### Review of the peer evaluation reports 

```{r include=FALSE}
## TODO: add numbers here based on peer_eval_review table
# Number of task1_analysis_pipeline remaining unacceptable after the review of peer evaluations
nrow(filter(peer_eval, task1_pipeline_acceptable == "(1) Unacceptable"))
```

Following the task 1 conclusion review, one full re-analysis was removed from the final dataset. For the remaining task 1 analysis pipeline review responses, all (n = 7) were revised to ‘(2) Acceptable but low quality’.

Following the task 1 categorization review , all initial ratings of ‘(2) Inadequate’ (n = 36) were revised to ‘Adequate’. In many cases, evaluators made their judgment of ‘inadequate’ on the basis of their task 1 conclusion rating. Put simply, evaluators often considered the categorization of results to be inadequate when they also judged that the conclusion does not follow from the results. It was often the case then, that verifying the legitimacy of the task 1 conclusion also verified the legitimacy of the task 1 categorization. The reasoning of the expert panel on a case-by-case basis can be found in the review supplement.

Following the task 2 analysis pipeline review, all initial ratings of ‘(1) Unacceptable’ (n = 17) were revised to ‘(2) Acceptable but low quality’. All initial ratings of ‘(5) Incomplete or missing analysis’ (n = 22) were also revised. Many of these ratings were made simply because the re-analysts task 1 submission also satisfied the requirements of task 2 (i.e., the paper-specific instructions given in task 2 had already been adhered to in task 1), and as a result, no further analysis was needed. For each case, the panel verified that the analyst had reported their test statistic appropriately in the task 2 survey response, and that their analysis files had been uploaded to the OSF as requested. 

Finally, there were no changes made to initial ratings following the code mismatches review. In the cases where evaluators reported ‘(4) I executed it and found mismatches’ (n = `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)`), the panel verified that the mismatches did not have a meaningful impact on the re-analyst’s reported conclusion, categorization or effect size.

## Inferential robustness: The robust of the conclusions to analytical choices published in social sciences

Do different analysts arrive at the same conclusions as the analysts of the original study?

### Task 1 Survey results

```{r include=FALSE}
# Distinct values of task1_categorisation
distinct(processed, task1_categorisation)
distinct(processed, task1_categorisation_plotting)

conclusions_main_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = simplified_paper_id, categorization_var = categorisation) |>
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(categorisation == "Same conclusion", percentage, NA), .desc = FALSE, .na_rm = TRUE)
  )

conclusions_main_robustness_data <- calculate_conclusion_robustness(data = processed, categorization_var = task1_categorisation_plotting)
```

In Task 1, the co-analysts were asked to conduct any statistical analysis to arrive at a single conclusion. Out of `r distinct(conclusions_main_robustness_data, N) |> pull(N)` re-analysed studies, the conclusions of `r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(n)` (`r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(n)`%) remained robust to independent re-analysis, so all assigned co-analysts arrived at the same conclusion as reported in the article of the original study (inferential robustness; see @fig-conclusions-main-robustness).

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-main-robustness
#| fig-cap: "The figure shows the proportion of the inferentially robust and not robust studies."

conclusion_main_robustness_plot <- plot_conclusion_robustness(conclusions_main_robustness_data, robust)

ggsave(here::here("figures/conclusion_main_robustness_plot.jpg"), conclusion_main_robustness_plot, dpi = 300)

conclusion_main_robustness_plot
```

@fig-conclusions-main shows the histogram display of the different and identical conclusions resulting from the re-analysis of each of the studies.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-main
#| fig-cap: "The figure shows the percentage of identical, inconclusive, and different conclusions for each study. Study numbers correspond to studies listed in Table S1."

conclusion_main_plot <-
  plot_percentage(
    data = conclusions_main_data,
    grouping_var = simplified_paper_id,
    categorization_var = categorisation,
    x_lab = "Study number",
    y_lab = "Percentage of re-analyses",
    with_sum = FALSE,
    reverse = TRUE,
    rev_limits = FALSE,
    coord_flip = TRUE
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 10),
    axis.title.x = element_text(size = 15),
    legend.justification = "left",
    legend.box = "horizontal",
    legend.position = "bottom",
    legend.text = element_text(size = 10)
  )

# Using a hacky solution to move legend under the Y axis
# conclusion_main_plot_wo_legend <- conclusion_main_plot + theme(legend.position = "none")
# legend <- cowplot::get_legend(conclusion_main_plot)
# conclusion_main_plot_w_legend <- cowplot::plot_grid(conclusion_main_plot_wo_legend, legend, nrow = 2, rel_heights = c(1, 0.05), rel_widths = c(1, 5))

ggsave(here::here("figures/conclusion_main_plot.jpg"), conclusion_main_plot, width = 9, height = 11.69, dpi = 300)

conclusion_main_plot
```

```{r include=FALSE}
conclusions_analysis_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  count(categorisation) |> 
  ungroup() |> 
  mutate(
    N = sum(n),
    freq = n / N,
    percentage = round(freq * 100, 2)
  )
```

Across all the re-analyses, `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(percentage)` % (`r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(N)`) of them arrived at the same conclusion; `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(N)`) to no effects, and `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(N)`) to opposite effect compared to the original conclusion.

#### Inferential robustness by discipline

We were interested to see whether the above results show a different pattern when inspecting them in different disciplines. @fig-discipline-robustness shows that for the fields with more than 10 studies in our collection (Economics, Political Science, and Psychology) the pattern was comparably similar. We found no outstanding differences between the discipline regarding the percentage of different and identical conclusions either (see @fig-conclusions-discipline).

```{r include=FALSE}
conclusions_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = paper_discipline, categorization_var = categorisation)

conclusions_discipline_robustness_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  calculate_conclusion_robustness(grouping_var = paper_discipline, categorization_var = task1_categorisation_plotting)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-discipline-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by major disciplines (more than 10 studies in our collection)."

conclusions_discipline_robustness_plot <-
  plot_percentage(
    data = conclusions_discipline_robustness_data,
    categorization_var = robust,
    grouping_var = paper_discipline,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "Disciplines"
  )

ggsave(here::here("figures/conclusions_discipline_robustness_plot.jpg"), conclusions_discipline_robustness_plot, dpi = 300)

conclusions_discipline_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-discipline
#| fig-cap: "The figure shows the percentage of identical, inconclusive, and different conclusions of the studies by major disciplines. The figure displays the count of re-analyses next to each discipline name."

conclusions_discipline_plot <-
  plot_percentage(
    data = conclusions_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Disciplines"
  ) +
  ggplot2::theme(
    legend.text = ggplot2::element_text(size = 8)
  )

ggplot2::ggsave(here::here("figures/conclusions_discipline_plot.jpg"), conclusions_discipline_plot, dpi = 300)

conclusions_discipline_plot
```

#### Inferential robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type. @fig-studytype-robustness illustrates that nearly half of the results from experimental studies remained robust upon independent re-analysis, whereas only one-third of observational studies yielded robust conclusions. Moreover, @fig-conclusions-studytype indicates that, for both study types, the majority of the re-analyses reached the same conclusions as the original study. 

```{r include=FALSE}
conclusions_studytype_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(categorisation = forcats::fct_relevel(
    categorisation,
    c("Same conclusion", "No effect/inconclusive", "Opposite effect")
  )) |>
  calculate_conclusion(grouping_var = experimental_or_observational,
                       categorization_var = categorisation) |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational))

conclusions_studytype_robustness_data <-
  processed |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational)) |>
  calculate_conclusion_robustness(grouping_var = experimental_or_observational,
                                  categorization_var = task1_categorisation_plotting) |>
  dplyr::ungroup()
```

```{r echo=FALSE, message=FALSE}
#| label: fig-studytype-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by study type (experimental or observational). The figure displays the count of re-analyses next to each study type name."

conclusions_studytype_robustness_plot <-
  plot_percentage(
    data = conclusions_studytype_robustness_data,
    categorization_var = robust,
    grouping_var = experimental_or_observational,
    y_lab = "Percentage of studies",
    x_lab = "Study type",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_robustness_plot.jpg"), conclusions_studytype_robustness_plot, dpi = 300)

conclusions_studytype_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-studytype
#| fig-cap: "The figure shows percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by study type (experimental, observational)."

conclusions_studytype_plot <-
  plot_percentage(
    data = conclusions_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = categorisation,
    x_lab = "Study type",
    y_lab = "Percentage of re-analyses",
    with_labels = TRUE
  ) +
  ggplot2::theme(legend.text = element_text(size = 8))

ggplot2::ggsave(here::here("figures/conclusions_studytype_plot.jpg"), conclusions_studytype_plot, dpi = 300)

conclusions_studytype_plot
```

#### Inferential robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts. @fig-conclusions-expertise shows these results. 

```{r include=FALSE, message=FALSE}
conclusions_expertise_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  calculate_conclusion(grouping_var = expertise_self_rating, categorization_var = categorisation) |> 
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    ),
    expertise_self_rating = dplyr::case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-expertise
#| fig-cap: "The figure shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by self-rated expertise (on a scale of 1 (Beginner) to 10 (Expert)). The figure does not display the bottom two categories where fewer than 3 responses were collected for each."

conclusions_expertise_plot <-
  plot_height(
    data = conclusions_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = categorisation,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    with_sum = TRUE,
    rev_limits = FALSE
  ) +
  theme(legend.text = element_text(size = 8))

ggsave(here::here("figures/conclusions_expertise_plot.jpg"), conclusions_expertise_plot, 
       height = 6, width = 8,
       dpi = 300)

conclusions_expertise_plot
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
conclusions_expertise_percentage_plot <-
  plot_percentage(
    data = conclusions_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = categorisation,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    with_sum = TRUE,
    rev_limits = FALSE
  ) +
  theme(legend.text = element_text(size = 8))

ggsave(here::here("figures/conclusions_expertise_percentage_plot.jpg"), conclusions_expertise_percentage_plot, 
       height = 6, width = 8,
       dpi = 300)

conclusions_expertise_percentage_plot
```

#### Inferential robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-conclusions-familiarity shows that for these results.

```{r include=FALSE}
count(processed, familiar_with_paper)

conclusions_familiarity_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |>
  mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    familiar_with_paper = as.factor(familiar_with_paper)
    ) |> 
  calculate_conclusion(grouping_var = familiar_with_paper, categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-familiarity
#| fig-cap: "The figure shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by declared familiarity with the study."

conclusions_familiarity_plot <- 
  plot_percentage(
    data = conclusions_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Analyst familiarity with the paper") + 
  theme(
    legend.text = element_text(size = 8)
    )

ggsave(here::here("figures/conclusions_familiarity_plot.jpg"), conclusions_familiarity_plot, dpi = 300)

conclusions_familiarity_plot
```

#### Inferential robustness by the level of confidence with the suitability of the analysis

```{r include=FALSE}
conclusions_suitability_data <- 
  processed |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
    ) |> 
  calculate_conclusion(grouping_var = confidence_in_approach, categorization_var = categorisation)
```

The following table shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by the analyst's level of confidence with the suitability of the analysis.

```{r echo=FALSE, message=FALSE}
conclusions_suitability_data |>
  dplyr::mutate(Count = paste(n, "/", N),
                percentage = paste0(percentage, "%")) |>
  dplyr::select(
    `Confidence rating` = confidence_in_approach,
    `Direction of the conclusion` = categorisation,
    Count,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

#### Inferential robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. @fig-conclusions-samplesize shows that for that..., 

```{r include=FALSE}
processed |>
  ggplot2::ggplot() +
  # Converting to natlog because of extreme outlier
  ggplot2::aes(x = log(reanalysis_model_sample_size)) +
  ggplot2::geom_histogram()

processed |> 
  dplyr::count(is.na(reanalysis_model_sample_size))
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-samplesize
#| fig-cap: "This raincloud figure shows the distribution of the sample sizes of the re-analyses resulting in same conclusion, no effect/inconclusive, and opposite effects."

conclusions_samplesize_plot <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_model_sample_size) |>
  # TODO: There are missing sample size values what to do with them?
  dplyr::filter(!is.na(reanalysis_model_sample_size)) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    )
  ) |>
  plot_rain(
    grouping_var = categorisation,
    response_var = reanalysis_model_sample_size,
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000),
    x_lab = "Direction of the re-analysis conclusion\ncompared to the original effect",
    y_lab = "Sample size"
  )

ggplot2::ggsave(here::here("figures/conclusions_samplesize_plot.jpg"), conclusions_samplesize_plot, dpi = 300)

conclusions_samplesize_plot
```

#### Estimate robustness: robust of the statistical findings published in social sciences to analytical choices

```{r include=FALSE}
# Number of cases where the original effect size is missing
missing_original_n <-
  processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(is.na(original_cohens_d)) |> 
  dplyr::filter(`is.na(original_cohens_d)`) |> 
  dplyr::pull(n)

# Check if there are cases where there is missing original effect size on the paper level but not for every analysis
# Would mean that there is a mistake in the merging code
processed |> 
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::group_by(paper_id) |> 
  dplyr::mutate(
    number_of_analysis = dplyr::n() 
  ) |> 
  dplyr::filter(is.na(original_cohens_d)) |> 
  mutate(
    number_of_analysis_after_filtering = dplyr::n()
  ) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(
    match = dplyr::if_else(number_of_analysis == number_of_analysis_after_filtering, TRUE, FALSE)
  )

# Number of cases where the reanalysed effect size is missing
missing_reanalysis_n <-
  processed |> 
  dplyr::count(is.na(reanalysis_cohens_d)) |> 
  dplyr::filter(`is.na(reanalysis_cohens_d)`) |> 
  dplyr::pull(n)

# Are there any cases where the original effect size is present but the reanalyzed is not and vica versa?
processed |> 
  dplyr::select(original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::mutate(
    both_present_or_missing = is.na(original_cohens_d) == is.na(reanalysis_cohens_d)
  ) |> 
  dplyr::count(both_present_or_missing)

# Check if there are any cases where everything is missing
processed |> 
  dplyr::group_by(paper_id) |>
  dplyr::mutate(
    both_missing = all(is.na(original_cohens_d) & is.na(reanalysis_cohens_d))
  ) |>
  dplyr::ungroup() |> 
  dplyr::count(both_missing)
  
# Preparing the data for the plot
# TODO: they want two colors and color legend with fixed position
reanalysis_data <-
  processed |>
  dplyr::select(simplified_paper_id,
                analyst_id,
                reanalysis_cohens_d,
                original_cohens_d) |>
  dplyr::group_by(simplified_paper_id) |>
  dplyr::rename(effect_size = reanalysis_cohens_d) |>
  dplyr::mutate(effect_size_type = paste0("re-analysis_0", row_number()),) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    # Put missing values at the end
    original_cohens_d = dplyr::if_else(is.na(original_cohens_d), -Inf, original_cohens_d),
    simplified_paper_id = forcats::fct_reorder(.f = simplified_paper_id,
                                               .x = original_cohens_d,
                                               .fun = function(x) median(x),
                                               .na_rm = FALSE)
  )

original_data <-
  processed |>
  dplyr::distinct(simplified_paper_id, original_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = forcats::fct_reorder(simplified_paper_id,
                                               original_cohens_d,
                                               .na_rm = FALSE)
  )

excluded_es <-
  dplyr::filter(reanalysis_data, effect_size > 10 | effect_size < -10) |> 
  mutate(excluded = glue::glue("study {simplified_paper_id}: {effect_size}")) |> 
  summarise(excluded_list = glue::glue_collapse(excluded, sep = "; "))
```

A main question of our study was whether different analysts arrive at the same effect estimates (+/- 0.05 Cohen’s d) as the analyst of the original study? Figure 18 shows the distribution of the effect sizes of the original and next results. @fig-effect-main shows percentages of the effect sizes falling within the preset tolerance range (+/- 0.05 Cohen’s d) for each study. For `r missing_reanalysis_n` re-analyses, the reported effect size was not convertible to Cohen’s. The figure does not show `r dplyr::filter(reanalysis_data, effect_size > 10 | effect_size < -10) |> nrow()` (`r dplyr::pull(excluded_es, excluded_list)`) re-analyzed effect sizes which are over 10 or smaller than -10 Cohen's d.  For `r missing_original_n` studies, we could not determine the original effect size due to missing information.

```{r echo=FALSE, warning=FALSE}
#| label: fig-effect-main
#| fig-cap: "The figure shows the effect size of the original result (black square) and the effect sizes of the re-analyses (green dot) for each study after conversions to Cohen’s d. Study numbers correspond to studies listed in Table S1."

# Colors
# color_vector <- setNames(c("#F8766D", "#CD9600", "#7CAE00", "#00BE67", "#00BFC4", "#00A9FF", "#C77CFF"), paste0("re-analysis_", sprintf("%02d", 1:7)))

effect_main_plot <-
  reanalysis_data |> 
  # TODO: what to do with huge cohens ds?
  dplyr::filter(effect_size <= 10 & effect_size >= -10) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    y = simplified_paper_id,
    x = effect_size,
    # color = effect_size_type
  ) +
  ggplot2::geom_point(
    shape = 16,
    color = viridis::viridis(5)[[3]],
    # size = 5
    ) +
  # ggplot2::scale_color_manual(values = color_vector) +
  ggplot2::geom_pointrange(
    data = original_data,
    ggplot2::aes(
      x = original_cohens_d,
      xmin = tolarence_region_lower,
      xmax = tolarence_region_upper,
      alpha = 0.8
      ),
    show.legend = FALSE,
    color = "black",
    shape = 15,
    # size = 2
    ) +
  labs(
    x = "Effect size in Cohen's d",
    y = "Study number"
  ) +
  ggplot2::guides(color = "none") +
  ggplot2::theme(
    axis.ticks = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(),
    plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10, "pt"),
    panel.grid = ggplot2::element_line(color = "lightgray"),
    panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.title = ggplot2::element_text(size = 20),
    axis.text.y = ggplot2::element_text(size = 10),
    axis.text.x = ggplot2::element_text(size = 13)
    # axis.text.y=element_text(margin = margin(1, unit = "cm"), vjust =1.5)
    )

# TODO: for some reason tolerance region is not showing on saved plot... why?
ggplot2::ggsave(here::here("figures/effect_main_plot.jpg"), plot = effect_main_plot,
       width = 12, height = 13,
       dpi = 300)

effect_main_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-all
#| fig-cap: "The figure shows percentages of the effect sizes falling within the preset tolerance range (+/- 0.05 Cohen’s d) for each study. Study numbers correspond to studies listed in Table S1."

effect_region_all_data <- 
  processed |> 
  calculate_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )
  
# TODO: replace this function and delete it from utils
effect_region_all_plot <- plot_tolarence_region(data = effect_region_all_data, grouping_var = simplified_paper_id, y_lab = "Study number", x_lab = "Percentage of reanalysis results") +
  ggplot2::theme(
    axis.title = ggplot2::element_text(size = 15),
    legend.text = ggplot2::element_text(size = 13),
    axis.text.x = ggplot2::element_text(size = 13)
  )

ggplot2::ggsave(here::here("figures/effect_region_all_plot.jpg"), effect_region_all_plot, width = 8.27, height = 11.69, dpi = 300)

effect_region_all_plot
```

```{r include=FALSE}
ind_within_tol_reg <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d >= tolarence_region_lower &
        reanalysis_cohens_d <= tolarence_region_upper ~ "Within tolerance region",
      reanalysis_cohens_d < tolarence_region_lower |
        reanalysis_cohens_d > tolarence_region_upper ~ "Outside of tolerance region",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(
      is_within_region,
      levels = c("Within tolerance region", "Outside of tolerance region")
    )
  ) |> 
  # Exclude missing tolerance region decisions
  dplyr::filter(!is.na(is_within_region)) |> 
  calculate_percentage(is_within_region)
  
within_tolerance_region <-
  effect_region_all_data |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & relative_frequency == 1), "Inferentially robust", "Inferentially not Robust")
    ) |> 
  dplyr::ungroup() |> 
  dplyr::count(robust) |> 
  dplyr::mutate(
    N = sum(n),
    relative_frequency = n / N,
    percentage = round(relative_frequency * 100)
    )
```

We found that `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(percentage)`% (`r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(n)` out of `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(N)`) of the studies contained at least one re-analysis result where the effect size was beyond the tolerance region (+/- 0.05 Cohen's d) of the result of the original study. Out of the `r dplyr::distinct(ind_within_tol_reg, N) |> dplyr::pull(N)` available reanalysis effect sizes `r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(percentage)`% (`r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(n)`) were outside of the tolerance region.

##### Estimate robustness by discipline

We were interested to see whether these robustness results show a different pattern when inspecting them in different disciplines. @fig-effect-region-discipline and @fig-effect-robustness-discipline show that for the major disciplines (>=10 studies).

```{r include=FALSE, warning=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(paper_discipline) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-discipline
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by major disciplines. The figure displays the count of re-analyses next to each discipline name."

effect_region_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |>
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  # Exclude missing values
  calculate_tolerance_region(grouping_var = paper_discipline, drop_missing = TRUE)

effect_region_discipline_plot <-
  plot_percentage(
    data = effect_region_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "Disciplines",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_discipline_plot.jpg"), effect_region_discipline_plot, dpi = 300)

effect_region_discipline_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-discipline
#| fig-cap: "This raincloud figure shows for each major discipline the distribution of effect size estimate ranges (lowest to highest) calculated per study."

effect_robustness_discipline_data <-
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  calculate_estimate_range(grouping_var = paper_discipline)

effect_robustness_discipline_plot <-
  plot_rain(data = effect_robustness_discipline_data,
            grouping_var = paper_discipline,
            response_var = estimate_range,
            x_lab = "Disciplines",
            y_lab = "Effect size estimate range in Cohen's d",
            trans = "log10",
            breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60))

ggplot2::ggsave(here::here("figures/effect_robustness_discipline_plot.jpg"), effect_robustness_discipline_plot, dpi = 300)

effect_robustness_discipline_plot 
```

##### Estimate robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type.

```{r include=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(experimental_or_observational) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-studytype
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by study type. The figure displays the count of re-analyses next to each discipline name."

effect_region_studytype_data <- 
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_tolerance_region(grouping_var = experimental_or_observational, drop_missing = TRUE)

effect_region_studytype_plot <-
  plot_percentage(
    data = effect_region_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "Study type",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_studytype_plot.jpg"), effect_region_studytype_plot, dpi = 300)

effect_region_studytype_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-studytype
#| fig-cap: "This raincloud figure shows for each study type the distribution of effect size estimate ranges (lowest to highest) calculated per study."

effect_robustness_studytype_data <-
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_estimate_range(grouping_var = experimental_or_observational)

effect_robustness_studytype_plot <- plot_rain(
  data = effect_robustness_studytype_data,
  grouping_var = experimental_or_observational,
  response_var = estimate_range,
  x_lab = "Study type",
  y_lab = "Effect size estimate range in Cohen's d",
  trans = "log10",
  breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)
)

ggplot2::ggsave(here::here("figures/effect_robustness_studytype_plot.jpg"), effect_robustness_studytype_plot, dpi = 300)

effect_robustness_studytype_plot 
```

##### Estimate robustness by expertise

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts.

```{r include=FALSE}
processed |> 
  dplyr::distinct(expertise_self_rating)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-expertise
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by self-rated expertise (on a scale of 1 (Beginner) to 10 (Expert)). The figure displays the count of re-analyses next to each discipline name."

effect_region_expertise_data <- 
  processed |> 
  dplyr::mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(expertise_self_rating, levels = c(
      "1\n(Beginner)",
      as.character(2:9),
      "10\n(Expert)")
    )
  ) |> 
  calculate_tolerance_region(grouping_var = expertise_self_rating, drop_missing = TRUE)

effect_region_expertise_plot <-
  plot_height(
    data = effect_region_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    rev_limits = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_expertise_plot.jpg"), effect_region_expertise_plot, dpi = 300,
                width = 8, height = 6
                )

effect_region_expertise_plot
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
effect_region_expertise_percentage_plot <-
  plot_percentage(
    data = effect_region_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    rev_limits = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_expertise_percentage_plot.jpg"), effect_region_expertise_percentage_plot, dpi = 300,
                width = 8, height = 6
                )

effect_region_expertise_percentage_plot
```

##### Estimate robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-effect-region-familiarity shows that for these results.

```{r echo=FALSE, message=FALSE, message=FALSE}
#| label: fig-effect-region-familiarity
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by declared familiarity with the study."

effect_region_familiarity_data <- calculate_tolerance_region(data = processed, grouping_var = familiar_with_paper, drop_missing = TRUE)

effect_region_familiarity_plot <-
  plot_percentage(
    data = effect_region_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis estimates in Cohen's d",
    x_lab = "Familiar with the paper",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_familiarity_plot.jpg"), effect_region_familiarity_plot, dpi = 300)

effect_region_familiarity_plot
```

##### Estimate robustness by the level of confidence with the suitability of the analysis 

```{r include=FALSE}
processed |> 
  dplyr::distinct(confidence_in_approach)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-suitability
#| fig-cap: "The figure shows "

effect_region_suitability_data <- calculate_tolerance_region(data = processed, grouping_var = confidence_in_approach, drop_missing = TRUE) |> 
  dplyr::mutate(
        confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
  )

effect_region_suitability_plot <-
  plot_height(
    data = effect_region_suitability_data,
    grouping_var = confidence_in_approach,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Level of confidence with the suitability of the analysis",
    with_labels = TRUE,
    rev_limits = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_suitability_plot.jpg"), effect_region_suitability_plot, dpi = 300, 
                width = 8, height = 6
                )

effect_region_suitability_plot
```

##### Estiamte robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following @fig-samplesize-region shows no remarkable differences between the two categories.

```{r echo=FALSE, message=FALSE}
#| label: fig-samplesize-region
#| fig-cap: "The figure shows the distribution of sample sizes separately for re-analysis effect sizes falling within or outside of the tolerance region of the original results. In this figure, we did not include those studies where the original effect sizes were missing, and cases where the re-analysis effect size or sample size were missing."

samplesize_region_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_model_sample_size,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d <= tolarence_region_lower |
        reanalysis_cohens_d >= tolarence_region_upper ~ "No",
      reanalysis_cohens_d >= tolarence_region_lower |
        reanalysis_cohens_d <= tolarence_region_upper ~ "Yes",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(is_within_region, levels = c("Yes", "No"))
  ) |> 
  dplyr::filter(
    !is.na(original_cohens_d),
    !is.na(reanalysis_model_sample_size),
    is_within_region != "Missing")


samplesize_region_plot <-
  plot_rain(
    data = samplesize_region_data,
    grouping_var = is_within_region,
    response_var = reanalysis_model_sample_size,
    y_lab = "Sample size",
    x_lab = "Is the re-analysis effect size within the tolerance region?",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  )

ggplot2::ggsave(here::here("figures/effect_region_samplesize_plot.jpg"), samplesize_region_plot, dpi = 300)

samplesize_region_plot
```

