---
title: "General descriptives"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggrain)

source(here::here("R/utils.R"))

processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

# Add number of evaluations per analysis
peer_eval <-
  peer_eval %>% 
  group_by(paper_id, analyst_id) %>% 
  mutate(n_evaluations = n()) %>% 
  ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

processed <- 
  processed %>% 
    mutate(cohen_analyst = runif(nrow(processed), min = 0.01, max = 0.99)) %>%
  group_by(paper_id) %>% 
  mutate(cohen_original = runif(1, min = 0.01, max = 0.99)) %>% 
  ungroup()

processed <-
  processed %>% 
  mutate(
    # Transforming the timestamp to date type from character
    task1_timestamp = lubridate::ymd_hms(task1_timestamp)
    )
```

Check if the analyst_id's are always unique to one person.

```{r}
processed %>%
  distinct(first_name, last_name, analyst_id) %>%  
  group_by(first_name, last_name) %>% 
  mutate(n_analyst_id = n()) %>% 
  arrange(desc(n_analyst_id))
```

## General

```{r echo=FALSE}
analyst_signed_up <- 
  all_people %>% 
  mutate(
    first_name = tolower(first_name),
    last_name = tolower(last_name)
  ) %>% 
  distinct(first_name, last_name, .keep_all = T) %>% 
  filter(disclosure_agreement == "I agree") %>% 
  nrow()

analyst_submitted <-
  processed %>% 
  distinct(analyst_id) %>% 
  nrow()
```

As a response to our recruitment call, `r analyst_signed_up` researchers signed up to participate in our study. Out of these volunteers, `r analyst_submitted` signed up to analyse at least one datasets and submitted their work by the deadline or an extended deadline.

```{r}
n_analysis <-
  processed %>% 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of co-analysts as some co-analysts volunteered to analyse more than one dataset.

```{r}
n_failed_peer <-
  processed %>% 
  filter(!peer_eval_pass) %>% 
  nrow()
```

Out of the submitted analyses, \_\_\_ were withdrawn, and \_\_\_ were omitted from the summary analysis for the following reasons: `r n_failed_peer` analyses failed the peer evaluation.

```{r}
final_n_analysis <-
  processed %>% 
  filter(peer_eval_pass) %>% 
  nrow()

final_n_analyst <-
  processed %>% 
  filter(peer_eval_pass) %>% 
  distinct(analyst_id) %>% 
  nrow()
```

As a result, we ended up with `r final_n_analysis` re-analyses, submitted by `r final_n_analyst` co-analysts.

## Task 1 Survey results

```{r}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position)

position <-
  processed %>%
  select(analyst_id, paper_id, current_position, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(current_position) %>% 
  rename(position = current_position)
```

Out of all the co-analysts who submitted their work by the deadline, there were `r filter(position, position == "Professor") %>% pull(n)` professors, `r filter(position, position == "Associate Professor") %>% pull(n)` associate professors, `r filter(position, position == "Post-Doc Researcher") %>% pull(n)` post-doctoral researchers, `r filter(position, position == "Doctoral Student") %>% pull(n)` doctoral students, `r filter(position, position == "Other academic/research position") %>% pull(n)` from other academic/research positions, and `r #filter(position, position == "Other") %>% pull(n)` from other positions.

```{r}
check_diff_response(processed, analyst_id, gender)

gender <-
  processed %>%
  select(analyst_id, paper_id, gender, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(gender)
```

The gender distribution of the co-analysts is as follows: `r filter(gender, gender == "Female") %>% pull(n)` female, `r filter(gender, gender == "Male") %>% pull(n)` male, `r filter(gender, gender == "Non-binary") %>% pull(n)` other, and `r filter(gender, gender == "Prefer not to say") %>% pull(n)` didn't want to respond to this question.

```{r}
check_diff_response(processed, analyst_id, age)

age <-
  processed %>%
  select(analyst_id, paper_id, age, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp)

age_group <-
  age %>% 
  mutate(
    age_group = case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    )
    ) %>% 
  count(age_group)
```

```{r}
age %>% 
  ggplot() +
  aes(x = age) +
  geom_histogram(stat="count") +
  scale_y_continuous(expand = c(0,0)) +
  labs(
    x = "Age",
    y = "Count"
  ) +
  theme(
    panel.grid = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line()
  )
```

The age distribution of the co-analysts is depicted in Fig X. `r filter(age_group, age == "young") %>% pull(n)` Young adults (-39 years); `r filter(age_group, age == "middle") %>% pull(n)` middle-ages adults (40-59 years); and `r filter(age_group, age == "old") %>% pull(n)` old adults (60- years).

```{r}
check_diff_response(processed, analyst_id, education_level)

education <-
  processed %>%
  select(analyst_id, paper_id, education_level, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(education_level) %>% 
  rename(education = education_level)
```

Regarding the highest level of education, `r filter(education, education == "") %>% pull(n)` co-analysts had Bachelor's degree or equivalent, `r filter(education, education == "") %>% pull(n)` Master's degree or equivalent, `r filter(education, education == "") %>% pull(n)` had Doctoral degree or equivalent; and `r filter(education, education == "") %>% pull(n)` reported other degree.

```{r}
check_diff_response(processed, analyst_id, country_of_residence)

country <- 
  processed %>%
  select(analyst_id, paper_id, country_of_residence, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(country_of_residence) %>% 
  rename(country = country_of_residence)
```

The country of residence of the co-analysts is shown on the map on Fig X. Regarding continents, \_\_ co-analysts were from Africa, \_\_\_ from Asia, \_\_\_ from Australia, \_\_\_ from Europe, \_\_\_ from North America, \_\_\_ from South America.

```{r}
check_diff_response(processed, analyst_id, analyst_discipline)

analyst_discipline <- 
  processed %>%
  select(analyst_id, paper_id, analyst_discipline, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  count(analyst_discipline) %>%
  rename(discipline = analyst_discipline)
```

We asked the co-analysts which discipline is the closest to their research area. The following table (Table X) summarises the distribution of their disciplinary orientation. Co-analysts from \_\_\_ and \_\_\_ disciplines participated in the highest ratio in this study.

```{r}
check_diff_response(processed, analyst_id, years_of_experience)
  
analyst_experience_years_data <-
  processed %>%
  select(analyst_id, paper_id, years_of_experience, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>% 
  # Dropped because of faulty response
  filter(analyst_id != "RTX71")

analyst_experience_years_plot <-
  analyst_experience_years_data %>% 
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram()
```

The distribution of the years of experience with data analysis is depicted on Fig X. The median time of experience with data analysis was X years among our co-analysts.

```{r}
check_diff_response(processed, analyst_id, analysis_frequency)

analysis_frequency_count <-
  processed %>% 
  mutate(
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(analysis_frequency,
                                     c(
                                       "Daily",
                                       "2-3 times a week",
                                       "Once a week",
                                       "Once every two weeks",
                                       "Once a month",
                                       "Less than once a month"
                                       )
                                     )
  ) %>% 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count %>%
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity")
```

We asked our co-analysts of how regularly they perform data analysis. Fig X. shows that the most frequent category was `r filter(analysis_frequency_count, n == max(n)) %>% pull(analysis_frequency)`.

```{r}
expertise_self_rating_data <-
  processed %>% 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) %>%
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1 (Beginner)",
      expertise_self_rating == 10 ~ "10 (Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(expertise_self_rating,
                                     c(
                                       "1 (Beginner)",
                                       as.character(2:9),
                                       "10 (Expert)"
                                       )
                                     )
  ) %>% 
  count(expertise_self_rating)

# 
expertise_self_rating_plot <-
  expertise_self_rating_data %>%
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity")
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution on Fig X shows that the most prevalent answer was `r filter(expertise_self_rating_data, n == max(n)) %>% pull(expertise_self_rating)` .

```{r}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

All together, `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") %>% pull(N)`) co-analysts indicated that they were familiar with the paper that the provided dataset belongs to before beginning their work on the project.

```{r}
processed %>% 
  count(communication_check)
```

No co-analysts reported that they communicated about the details of their analysis with other co-analysts working with the same dataset.

```{r}
software_data <-
  processed %>% 
  select(task1_software, task2_software)
# prepsocessing
# pooled
# percentage
```

We asked the co-analysts what programming language/software/tool they used in their data analysis. The following figure indicates that \_\_ (X%), \_\_ (Y%), and \_\_ (Z%) were the most popular responses. FIGURE

## Task 2 Survey results

```{r}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the co-analysts to present one main statistical result, `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(percentage)`% of them (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(n)` our of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") %>% pull(N)`) based their conclusion on p-value and `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(percentage)`% of them (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(n)` our of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") %>% pull(N)`) used Bayes Factor.

```{r}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

A difference in Task 2 compared to Task 1 was that the co-analysts received some constraints for their analysis in order to make them linkable to a single result in the original study. `r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(percentage)` % (`r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") %>% pull(N)`) the co-analysts reported that they had to make additional calculations in the second task. In `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") %>% pull(N)`) the co-analysts indicated that despite the limitations in the instructions, they received the same result in Task 2 and Task 1.

```{r}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(n)` our of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") %>% pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r}
total_hours_data <-
  processed %>% 
  # exclusions in text
  filter(total_hours != 999)

total_hours_data %>% 
  ggplot() +
  aes(x = total_hours) +
  geom_histogram()
```

The co-analysts were asked to estimate the time they spent to perform Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (Fig X).

## Peer evaluation

### Peer evaluators

Basic demographic info.

```{r}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval %>% 
  distinct(evaluator_id) %>% 
  inner_join(., processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data %>% 
  count(evaluator_id) %>% 
  arrange(desc(n))
```

Experience with conducting statistical analysis:

```{r}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
  
peer_analyst_experience_years_data <-
  peer_evaluator_data %>%
  select(evaluator_id, years_of_experience, task1_timestamp) %>% 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data %>% 
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram()
```

Frequency of data analysis:

```{r}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)

peer_analysis_frequency_count <-
  peer_evaluator_data %>% 
  mutate(
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(analysis_frequency,
                                     c(
                                       "Daily",
                                       "2-3 times a week",
                                       "Once a week",
                                       "Once every two weeks",
                                       "Once a month",
                                       "Less than once a month"
                                       )
                                     )
  ) %>% 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <
  peer_analysis_frequency_count %>% 
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity")
```

Self-reported expertise in data-analysis:

```{r}
peer_expertise_self_rating_data <-
  peer_evaluator_data %>% 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp) %>%
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1 (Beginner)",
      expertise_self_rating == 10 ~ "10 (Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(expertise_self_rating, levels = c(
      "1 (Beginner)",
      as.character(2:9),
      "10 (Expert)")
    )
  ) %>% 
  count(expertise_self_rating) %>% 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data %>%
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity")
```

### Peer evaluations

Nr. of peer evaluations:

```{r}
nrow(peer_eval)
```

Descriptives of peer evaluations.

```{r}
# TODO: should I group the responses by acceptable and unacceptable first?
pipeline_acceptable_data <-
  peer_eval %>% 
  # Dropping analyses with less than 1 evaluation
  filter(n_evaluations > 1) %>% 
  group_by(paper_id, analyst_id) %>% 
  summarise(
    disagree_task1 = n_distinct(task1_pipeline_acceptable) > 1,
    disagree_task2 = n_distinct(task2_pipeline_acceptable) > 1,
  ) %>% 
  ungroup()

task1_pipeline_acceptable_data <- calculate_percentage(pipeline_acceptable_data, disagree_task1)

task2_pipeline_acceptable_data <- calculate_percentage(pipeline_acceptable_data, disagree_task2)
```

For those analyses where there were more than one peer evaluations, for `r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(percentage)`% (`r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(n)` out of `r filter(task1_pipeline_acceptable_data, disagree_task1) %>% pull(N)`) of the analysis the evaluators disagreed on the analytical pipeline for task 1, and `r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(percentage)`% (`r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(n)` out of `r filter(task2_pipeline_acceptable_data, disagree_task2) %>% pull(N)`) for task 2.

```{r}

```

\% (x out of y) of acceptable analysis pipelines (Task 1) - the outcome of the procedure, + % of peer evaluations we need to adjust,

```{r}

```

\% (x out of y) of acceptable analysis pipelines (Task 2) - the outcome of the procedure, + % of peer evaluations we need to adjust,

```{r}
task1_categorisation_is_accurate_data <-
  peer_eval %>% 
  # Dropping analyses with less than 1 evaluation
  filter(n_evaluations > 1) %>% 
  group_by(paper_id, analyst_id) %>% 
  summarise(
    disagree_task1 = n_distinct(task1_categorisation_is_accurate) > 1
  ) %>% 
  ungroup() %>% 
  calculate_percentage(disagree_task1)
```

For those analyses where there were more than one peer evaluator, `r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(percentage)`% (`r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(n)` out of `r filter(task1_categorisation_is_accurate_data, disagree_task1) %>% pull(N)`) of evaluators disagreed on the adequacy of the conclusions.

```{r}

```

\% (x out of y) of adequate conclusions (Task 1) - outcome of the procedure, + % of peer evaluations we need to adjust,

```{r}

```

\% (x out of y) of cases where the correction of the self-categorization of the conclusion was necessary

```{r}

```

Nr. of analytical reproducibility checks:

```{r}
reproducibility_checks_data <-
  peer_eval %>% 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_data %>% 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") %>% 
  summarise(n_reproducibility_checks = sum(n))
```

```{r}
reproducibility_checks_successful <-
  peer_eval %>% 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") %>% 
  mutate(successful = case_when(
    any_code_mismatches %in% c("(2) I tried but didn’t manage to execute it", "(4) I executed it and I found mismatches") ~ FALSE,
    any_code_mismatches == "(3) I executed it and I found no mismatches" ~ TRUE,
    )
  ) %>% 
  calculate_percentage(successful)
```

`r filter(reproducibility_checks_successful, successful) %>% pull(percentage)`% (`r filter(reproducibility_checks_successful, successful) %>% pull(n)` out of `r filter(reproducibility_checks_successful, successful) %>% pull(N)` of the analytical reproducibility checks were successful

## How robust are conclusions published in social sciences to analytical choices?

Do different analysts arrive at the same conclusions as the analysts of the original study?

### Task 1 Survey results

In Task 1, the co-analysts were asked to conduct any statistical analysis to arrive to a single conclusion. Out of \_\_ re-analysed studies, the conclusions of \_\_ (X%) remained robust to independent re-analysis, so that all assigned co-analysts arrived at the same conclusion as reported in the article of the original study. FIGURE
