---
title: "Results"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)

# Load custom functions
source(here::here("R/utils.R"))

# Read processed data files
processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

## Transform datafiles for analysis 
# Add number of evaluations per analysis
peer_eval <-
  peer_eval |>
  dplyr::group_by(paper_id, analyst_id) |>
  dplyr::mutate(n_peer_evals = dplyr::n()) |>
  dplyr::ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

# Transforming the timestamp to date type from character
processed <-
  processed |>
  dplyr::mutate(
    task1_timestamp = lubridate::ymd_hms(task1_timestamp))
```

```{r include=FALSE}
# Check if the analyst_id's are always unique to one person
processed |>
  dplyr::distinct(first_name, last_name, analyst_id) |>
  dplyr::group_by(first_name, last_name) |>
  dplyr::mutate(n_analyst_id = dplyr::n()) |>
  dplyr::arrange(dplyr::desc(n_analyst_id))
```

## General descriptives

```{r include=FALSE}
analyst_signed_up <-
  all_people |>
  dplyr::mutate(first_name = tolower(first_name),
                last_name = tolower(last_name)) |>
  dplyr::distinct(first_name, last_name, .keep_all = T) |>
  dplyr::filter(disclosure_agreement == "I agree")

analyst_submitted <-
  processed |>
  dplyr::distinct(analyst_id) |>
  nrow()
```

As a response to our recruitment call, `r nrow(analyst_signed_up)` researchers signed up to participate in our study. Out of these volunteers, `r analyst_submitted` signed up to analyse at least one dataset and submitted their work by the deadline or an extended deadline.

```{r include=FALSE}
n_analysis <-
  processed |> 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of co-analysts as some co-analysts volunteered to analyse more than one dataset.

```{r include=FALSE}
n_failed_peer <-
  processed |>
  dplyr::filter(!peer_eval_pass) |>
  nrow()
```

Out of the submitted analyses `r nrow(dplyr::filter(processed, !peer_eval_pass))` were omitted from the summary analysis their analysis failed the peer evaluation.

```{r include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  dplyr::filter(peer_eval_pass)
```

```{r include=FALSE}
final_n_analyst <-
  processed |> 
  dplyr::distinct(analyst_id) |> 
  nrow()
```

As a result, we ended up with `r nrow(processed)` re-analyses, submitted by `r final_n_analyst` co-analysts.

Although we invited more than 5 co-analysts to each of the 100 studies, due to drop-outs the final number of completed analyses ranged between `r min(dplyr::count(processed, paper_id)$n)` and `r max(dplyr::count(processed, paper_id)$n)`. Table 1 shows the the distribution of analyses per studies. 

```{r echo=FALSE, message=FALSE}
processed |>
  dplyr::count(paper_id) |> 
  dplyr::count(n, name = "N") |>
  dplyr::rename(`Number of Completed Analyses` = n,
                `Number of Studies` = N) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

## Basic demographics of the co-analysts

```{r include=FALSE}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position_grouped)

position <-
  processed |>
  dplyr::select(analyst_id, paper_id, current_position_grouped, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) |> 
  count(current_position_grouped) |> 
  rename(position = current_position_grouped)
```

Out of all the co-analysts who submitted their work by the deadline, there were `r dplyr::filter(position, position == "Professor") |> dplyr::pull(n)` professors, `r dplyr::filter(position, position == "Associate Professor") |> dplyr::pull(n)` associate professors, `r filter(position, position == "Assistant Professor") |> dplyr::pull(n)` assistant professors, `r dplyr::filter(position, position == "Post-Doc Researcher") |> dplyr::pull(n)` post-doctoral researchers, `r dplyr::filter(position, position == "Doctoral Student") |> dplyr::pull(n)` doctoral students, `r dplyr::filter(position, position == "Other academic/research position") |> dplyr::pull(n)` from other academic/research positions, and `r dplyr::filter(position, position == "Other") |> dplyr::pull(n)` from other positions. 

```{r include=FALSE}
check_diff_response(processed, analyst_id, gender)

gender <-
  processed |>
  dplyr::select(analyst_id, paper_id, gender, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(gender)
```

The gender distribution of the co-analysts is as follows: `r dplyr::filter(gender, gender == "Female") |> dplyr::pull(n)` female, `r dplyr::filter(gender, gender == "Male") |> dplyr::pull(n)` male, `r dplyr::filter(gender, gender == "Non-binary") |> dplyr::pull(n)` other, and `r dplyr::filter(gender, gender == "Prefer not to say") |> dplyr::pull(n)` didn't want to respond to this question.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = age)

age <-
  processed |>
  dplyr::select(analyst_id, paper_id, age, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Filter erroneous data
  dplyr::filter(age != "00") |> 
  dplyr::mutate(age = as.numeric(age))
  
age_group <-
  age |> 
  dplyr::mutate(
    age_group = dplyr::case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    ),
    age_group = factor(age_group, levels = c("young", "middle", "old"))
    ) |> 
  dplyr::count(age_group) |> 
  tidyr::complete(age_group, fill = list(n = 0))
```

The age distribution of the co-analysts is depicted in @fig-age-plot. `r dplyr::filter(age_group, age_group == "young") |> dplyr::pull(n)` young adults (-39 years); `r dplyr::filter(age_group, age_group == "middle") |> dplyr::pull(n)` middle-aged adults (40-59 years); and no old adults (60- years).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-age-plot
#| fig-cap: "The figure shows the distribution of the analysts' age. When an analyst submitted more than one re-analysis with more than a year apart, we only kept their age at the time of their first submission. Moreover, one analyst was excluded because they did not disclose their age."
age_plot <- 
  age |>
  ggplot2::ggplot() +
  ggplot2::aes(x = age) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::scale_y_continuous(expand = c(0, 0), limits = c(0, 40)) +
  ggplot2::scale_x_continuous(
    # limits = c(20, 55),
    # breaks = c(20, 30, 40, 50, 60),
    # labels = c("20", "30", "40", "50", "60")
  ) +
  ggplot2::labs(x = "Age (years)",
       y = "Number of co-analysts") +
  ggplot2::theme(
    panel.grid = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.line = ggplot2::element_line()
  )

ggplot2::ggsave(here::here("figures/demographic_age_plot.jpg"), age_plot, dpi = 300)

age_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = education_level)

education <-
  processed |>
  dplyr::select(analyst_id, paper_id, education_level, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(education_level) |> 
  dplyr::rename(education = education_level)
```

Regarding the highest level of education, `r dplyr::filter(education, education == "High-school diploma or equivalent") |> dplyr::pull(n)` reported High-school diploma or equivalent, `r dplyr::filter(education, education == "Bachelor's degree or equivalent") |> dplyr::pull(n)` co-analysts had Bachelor's degree or equivalent, `r dplyr::filter(education, education == "Master's degree or equivalent") |> dplyr::pull(n)` Master's degree or equivalent, `r dplyr::filter(education, education == "Doctoral degree or equivalent") |> dplyr::pull(n)` had Doctoral degree or equivalent. In case the analysts completed more than one re-analysis and they advanced in their studies by the time of their second analysis, we only kept their first response for this comparison.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = country_of_residence)

country_data <- raster::ccodes()

country <- 
  processed |>
  dplyr::select(analyst_id, paper_id, country_of_residence, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(country_of_residence) |> 
  dplyr::rename(region = country_of_residence) |> 
  # Modify country names to fit the worldmap data
  dplyr::mutate(
    subregion = dplyr::case_when(
      region == "Hong Kong (China)" ~ "Hong Kong",
      TRUE ~ NA_character_
    ),
    region = dplyr::case_when(
      region == "Hong Kong (China)" ~ "China",
      region == "United States" ~ "USA",
      region == "United Kingdom" ~ "UK",
      TRUE ~ region
    ),
    continent = countrycode::countrycode(region, "country.name", "continent"),
    iso3_code = countrycode::countrycode(region, "country.name", "iso3c")
  ) |> 
  dplyr::left_join(dplyr::select(country_data, ISO3, UNREGION1), by = c("iso3_code" = "ISO3"))

continent <-
  country |> 
  dplyr::group_by(continent) |> 
  dplyr::summarise(N = sum(n))

region <-
  country |> 
  dplyr::group_by(UNREGION1) |> 
  dplyr::summarise(N = sum(n))
```

The country of residence of the co-analysts is shown on the map on @fig-country-plot. Regarding the continents, `r dplyr::filter(continent, continent == "Africa") |> dplyr::pull(N)` co-analyst was from Africa, `r dplyr::filter(continent, continent == "Asia") |> dplyr::pull(N)` were from Asia, `r dplyr::filter(continent, continent == "Oceania") |> dplyr::pull(N)` from Oceania, `r dplyr::filter(continent, continent == "Europe") |> dplyr::pull(N)` from Europe, `r dplyr::filter(region, UNREGION1 == "Northern America") |> dplyr::pull(N)` from North America, `r dplyr::filter(region, UNREGION1 %in% c("Central America", "South America")) |> dplyr::summarise(sum(N)) |> dplyr::pull("sum(N)")` from South America.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-country-plot
#| fig-cap: "The figure shows the analysts' country of residence. When an analyst submitted more than one re-analysis and they moved between the submissions, we only kept their first response."

# TODO: This is an ugly solution ask someone for a better one
world_map <- 
  ggplot2::map_data("world") |> 
  dplyr::mutate(
    subregion = dplyr::case_when(
      subregion == "Hong Kong" ~ subregion,
      TRUE ~ NA_character_
    )
  )

country_map <- dplyr::left_join(world_map, country, by = c("region", "subregion"))

country_map_plot <- 
  country_map |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = long, y = lat, group = group, fill = n) +
  ggplot2::geom_polygon(color = "white", linewidth = 0.2) +
  ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Number of\nanalyst") +
  ggplot2::theme_void()

ggplot2::ggsave(here::here("figures/demographic_country_plot.jpg"), country_map_plot, dpi = 300)

country_map_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = primary_discipline)

analyst_discipline <- 
  processed |>
  dplyr::select(analyst_id, paper_id, primary_discipline, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  calculate_percentage(response_var = primary_discipline) |>
  dplyr::rename(discipline = primary_discipline) |> 
  dplyr::arrange(dplyr::desc(dplyr::if_else(discipline == "Other", -Inf, percentage)))
```

We asked the co-analysts which discipline is the closest to their research area. The following Table summarizes the distribution of their disciplinary orientation. Co-analysts from `r dplyr::slice(analyst_discipline, 1) |> dplyr::pull(discipline)` and `r dplyr::slice(analyst_discipline, 2) |> dplyr::pull(discipline)` disciplines participated in the highest ratio in this study.

```{r, echo=FALSE}
# tbl-discipline
# "Distribution of the Analysts' Primary Discipline"

analyst_discipline |>
  dplyr::select(discipline, n, percentage) |>
  dplyr::rename(Discipline = discipline,
                Count = n,
                Percentage = percentage) |>
  gt::gt() |>
  tab_style(style = gt::cell_text(weight = "bold"),
            locations = gt::cells_column_labels()) |>
  gt::tab_footnote(
    "Note: Whenever the respondents provided more than one field we only kept their first responses."
  )
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = years_of_experience)

analyst_experience_years_data <-
  processed |>
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Dropped because of faulty response
  dplyr::filter(analyst_id != "RTX71")
```

The distribution of the years of experience with data analysis is depicted on @fig-experience-years-plot. The median time of experience with data analysis was `r median(analyst_experience_years_data$years_of_experience)` years among our co-analysts.

```{r echo=FALSE, message=FALSE}
#| label: fig-experience-years-plot
#| fig-cap: "The figure shows the analysts' years of experience with data analysis. When an analyst submitted more than one re-analysis and a year passed between the responses we only kept their first response."
analyst_experience_years_plot <-
  analyst_experience_years_data |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = years_of_experience) +
  ggplot2::geom_histogram() +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Years of experience with data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/demographic_experience_years_plot.jpg"), analyst_experience_years_plot, dpi = 300)

analyst_experience_years_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = analysis_frequency)

analysis_frequency_count <-
  processed |> 
  dplyr::select(analyst_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(analysis_frequency)
```

We asked our co-analysts how regularly they perform data analysis. @fig-analysis-frequency. shows that the most frequent category was `r dplyr::filter(analysis_frequency_count, n == max(n)) |> dplyr::pull(analysis_frequency)`.

```{r echo=FALSE, message=FALSE}
#| label: fig-analysis-frequency
#| fig-cap: "The figure shows how regularly the analysts perform data analysis."

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count |>
  dplyr::mutate(
    analysis_frequency = dplyr::case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = forcats::fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot2::ggplot() +
  ggplot2::aes(x = analysis_frequency, y = n) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Frequency of doing data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/demographic_analysis_frequency_plot.jpg"), analysis_frequency_plot, dpi = 300)

analysis_frequency_plot
```

```{r include=FALSE}
expertise_self_rating_data <-
  processed |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |>
  dplyr::count(expertise_self_rating)
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution on @fig-self-rating-plot shows that the most prevalent answer was `r dplyr::filter(expertise_self_rating_data, n == max(n)) |> dplyr::pull(expertise_self_rating)` .

```{r echo=FALSE, message=FALSE}
#| label: fig-self-rating-plot
#| fig-cap: "The figure shows the analysts' self-rated level of expertise in data analysis. When an analyst submitted more than one re-analysis we only kept their first response."

expertise_self_rating_plot <-
  expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(
      expertise_self_rating,
      c("1\n(Beginner)",
        as.character(2:9),
        "10\n(Expert)")
    )
  ) |>
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-rated expertise of data analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_expertise_self_rating_plot.jpg"), expertise_self_rating_plot, dpi = 300)

expertise_self_rating_plot
```

```{r include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

In `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(N)`) of the cases, the co-analysts were familiar with the paper that the provided dataset belongs to before beginning their work on the project.

```{r include=FALSE}
processed |> 
  count(communication_check)
```

All co-analyst reported that they have not communicated about the details of their analysis with other co-analysts working with the same dataset.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n)) |> 
  mutate(
    software = case_when(
      software %in% c("r", "stata", "spss", "jasp") ~ toupper(software),
      TRUE ~ stringr::str_to_title(software)
    )
  )
```

We asked the co-analysts what programming language/software/tool they used in their data analysis during Task 1 and Task 2. The following figure indicates that `r slice(software_data, 1) |> pull(software)` (`r slice(software_data, 1) |> pull(percentage)`%), `r slice(software_data, 2) |> pull(software)` (`r slice(software_data, 2) |> pull(percentage)`%), and `r slice(software_data, 3) |> pull(software)` (`r slice(software_data, 3) |> pull(percentage)`%) were the most popular responses. @fig-software shows the distribution of all the responses.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-software
#| fig-cap: "The figure shows which software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported the use of multiple software we kept all their responses for this figure. The figure shows only software that was used by more than 1% of the analysts."

software_plot <-
  software_data |>
  dplyr::filter(percentage > 1) |>
  ggplot() +
  aes(y = reorder(software, percentage),
      x = percentage) +
  geom_bar(stat = "identity") +
  scale_x_continuous(expand = c(0, 0),
                     labels = scales::percent_format(scale = 1)) +
  labs(x = "Percentage of co-analysts",
       y = "Software") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_software_plot.jpg"), software_plot, dpi = 300)

software_plot
```

## Descriptives of the statistical analyses

A difference in Task 2 compared to Task 1 was that the co-analysts received some constraints for their analysis in order to make them linkable to a single result in the original study (see Methods for more details). 

```{r include=FALSE}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the co-analysts to present one main statistical result, in `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(percentage)`% of the analyses (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(N)`), conclusion was based on p-value and Bayes Factor was used in `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(percentage)`% of the cases (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(N)`).

```{r include=FALSE}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

For `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(percentage)` % (`r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(N)`) of the analyses, the co-analysts reported that they had to make additional calculations in the second task. In the remaining `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(N)`) of the cases, the co-analysts indicated that despite the requirements of the instructions, they could conduct the same analyses as in Task 1.

```{r include=FALSE}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(n)` our of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r include=FALSE}
total_hours_data <-
  processed |> 
  filter(total_hours != 999)
```

The co-analysts were asked to estimate the time they spent performing Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (@fig-total-hours).

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-total-hours
#| fig-cap: "The figure shows the total hours the analyst spent on Task 1 and Task 2 together. In case an analyst completed multiple re-analyses, we kept all their responses for this figure. One response was excluded due to being an outlier (999 hours)."

total_hours_plot <-
  total_hours_data |>
  ggplot() +
  aes(x = total_hours) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Total hours spent on the analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_total_hours_plot.jpg"), total_hours_plot, dpi = 300)

total_hours_plot
```

## Peer evaluation
### Peer evaluators

Basic demographic info.

```{r include=FALSE}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval |> 
  distinct(evaluator_id) |> 
  inner_join(processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data |> 
  count(evaluator_id) |> 
  arrange(desc(n))
```

Experience with conducting statistical analysis:

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analyst_experience_years_data <-
  peer_evaluator_data |>
  dplyr::select(evaluator_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data |>
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_experience_years_plot.jpg"), peer_analyst_experience_years_plot, dpi = 300)

peer_analyst_experience_years_plot
```

Frequency of data analysis:

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analysis_frequency_count <-
  peer_evaluator_data |> 
  dplyr::select(evaluator_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |> 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <-
  peer_analysis_frequency_count |>
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of doing data analysis",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_analysis_frequency_plot.jpg"), peer_analysis_frequency_plot, dpi = 300)

peer_analysis_frequency_plot
```

Self-reported expertise in data-analysis:

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_expertise_self_rating_data <-
  peer_evaluator_data |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |>
  count(expertise_self_rating) |> 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) |>
  tidyr::complete(expertise_self_rating, fill = list(n = 0)) |> 
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-reported expertise rating",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_expertise_self_rating_plot.jpg"), peer_expertise_self_rating_plot, dpi = 300)

peer_expertise_self_rating_plot
```

### Peer evaluations

Nr. of peer evaluations:

```{r include=FALSE}
nrow(peer_eval)
```

Descriptives of peer evaluations.

```{r include=FALSE}
# TODO: should I group the responses by acceptable and unacceptable first?
pipeline_acceptable_data <-
  peer_eval |> 
  # Dropping analyses with less than 1 evaluation
  dplyr::filter(n_peer_evals > 1) |> 
  dplyr::group_by(paper_id, analyst_id) |> 
  dplyr::summarise(
    disagree_task1 = dplyr::n_distinct(task1_pipeline_acceptable) > 1,
    disagree_task2 = dplyr::n_distinct(task2_pipeline_acceptable) > 1,
  ) |> 
  dplyr::ungroup()

task1_pipeline_acceptable_data <- calculate_percentage(data = pipeline_acceptable_data, response_var = disagree_task1)

task2_pipeline_acceptable_data <- calculate_percentage(data = pipeline_acceptable_data, response_var = disagree_task2)
```

For those analyses where there were more than one peer evaluations, for `r dplyr::filter(task1_pipeline_acceptable_data, disagree_task1) |> dplyr::pull(percentage)`% (`r dplyr::filter(task1_pipeline_acceptable_data, disagree_task1) |> dplyr::pull(n)` out of `r dplyr::filter(task1_pipeline_acceptable_data, disagree_task1) |> dplyr::pull(N)`) of the analysis the evaluators disagreed on the analytical pipeline for task 1, and `r dplyr::filter(task2_pipeline_acceptable_data, disagree_task2) |> dplyr::pull(percentage)`% (`r filter(task2_pipeline_acceptable_data, disagree_task2) |> dplyr::pull(n)` out of `r dplyr::filter(task2_pipeline_acceptable_data, disagree_task2) |> dplyr::pull(N)`) for task 2.

```{r include=FALSE}
distinct(peer_eval, task1_pipeline_acceptable)

task1_acceptable_summary <-
  peer_eval |>
  dplyr:: mutate(
    is_acceptable = dplyr::case_when(
      task1_pipeline_acceptable == "(1) Unacceptable" ~ FALSE,
      task1_pipeline_acceptable %in% c(
        "(2) Acceptable but low quality",
        "(3) Acceptable and medium quality",
        "(4) Acceptable and high quality"
      ) ~ TRUE
    )
  ) |>
  calculate_percentage(response_var = is_acceptable)

# TODO: rewrite this part for safety
task1_pipline_acceptable_review <-
  peer_eval_review |>
  filter(variable_name == "task1_pipeline_acceptable" &
           !is.na(change_to))
```

`r dplyr::filter(task1_acceptable_summary, is_acceptable) |> dplyr::pull(percentage)`% (`r dplyr::filter(task1_acceptable_summary, is_acceptable) |> dplyr::pull(n)` out of `r dplyr::filter(task1_acceptable_summary, is_acceptable) |> dplyr::pull(N)`) of acceptable analysis pipelines (Task 1) - the outcome of the procedure

`r nrow(task1_pipline_acceptable_review)` of peer elevators' responses we need to adjust for this variable (comment from Marci: we only adjusted the unacceptable responses to acceptable but with low quality I think we can be explicit about that)

```{r include=FALSE}
distinct(peer_eval, task2_pipeline_acceptable)

task2_acceptable_summary <-
  peer_eval |>
  mutate(
    is_acceptable = case_when(
      task2_pipeline_acceptable == "(1) Unacceptable" ~ "unacceptable",
      task2_pipeline_acceptable %in% c(
        "(2) Acceptable but low quality",
        "(3) Acceptable and medium quality",
        "(4) Acceptable and high quality"
      ) ~ "acceptable",
      task2_pipeline_acceptable == "(5) Incomplete or missing analysis" ~ "incomplete",
      TRUE ~ NA_character_
    )
  ) |>
  calculate_percentage(response_var = is_acceptable)

# TODO: rewrite this part for safety
task2_pipline_acceptable_review <-
  peer_eval_review |>
  filter(variable_name == "task2_pipeline_acceptable" &
           !is.na(change_to))
```

`r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(percentage)`% (`r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(n)` out of `r filter(task2_acceptable_summary, is_acceptable == "acceptable") |> pull(N)`) analysis pipelines (Task 2) were acceptable, and `r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(percentage)`% (`r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(n)` out of `r filter(task2_acceptable_summary, is_acceptable == "incomplete") |> pull(N)`) analysis pipelines (Task 2) were incomplete

`r nrow(task2_pipline_acceptable_review)` responses of peer evaluations we need to adjust for this variable (comment form marci: again we only modified the unacceptable to acceptable)

```{r include=FALSE}
task1_categorisation_is_accurate_data <-
  peer_eval |> 
  # Dropping analyses with less than 1 evaluation
  filter(n_peer_evals > 1) |> 
  group_by(paper_id, analyst_id) |> 
  summarise(
    disagree_task1 = n_distinct(task1_categorisation_is_accurate) > 1
  ) |> 
  ungroup() |> 
  calculate_percentage(response_var = disagree_task1)
```

For those analyses where there were more than one peer evaluator, `r filter(task1_categorisation_is_accurate_data, disagree_task1) |> pull(percentage)`% (`r filter(task1_categorisation_is_accurate_data, disagree_task1) |> pull(n)` out of `r filter(task1_categorisation_is_accurate_data, disagree_task1) |> pull(N)`) of evaluators disagreed on the adequacy of the conclusions.

```{r include=FALSE}
distinct(peer_eval, task1_categorisation_is_accurate)

task1_categorisation_summary <- 
  peer_eval |> 
  calculate_percentage(response_var = task1_categorisation_is_accurate) |> 
  dplyr::mutate(
    task1_categorisation_is_accurate = stringr::str_remove(task1_categorisation_is_accurate, "\\(\\d+\\)\\s*")
  )

# TODO: rewrite this part for safety
task1_categorisation_review <-
  peer_eval_review |>
  dplyr::filter(variable_name == "task1_categorisation_is_accurate" &
           !is.na(change_to))
```

`r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(percentage)`% (`r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(n)` out of `r filter(task1_categorisation_summary, task1_categorisation_is_accurate == "Adequate") |> pull(N)`) the co-analyst’s self-categorization of the results was adequate.

`r nrow(task1_categorisation_review)` peer evaluations we need to adjust

```{r include=FALSE}
distinct(peer_eval, task1_conclusion_follows_results)

task1_conclusion_summary <- 
  peer_eval |> 
  calculate_percentage(task1_conclusion_follows_results) |> 
  mutate(
    task1_conclusion_follows_results = stringr::str_remove(task1_conclusion_follows_results, "\\(\\d+\\)\\s*")
  )

task1_conclusion_review <-
  peer_eval_review |>
  dplyr::filter(variable_name == "task1_conclusion_follows_results" &
           !is.na(change_to))
```

`r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(percentage)`% (`r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(n)` out of `r filter(task1_conclusion_summary, task1_conclusion_follows_results == "It adequately follows from the results of the analysis") |> pull(N)`) conclusions adequately followed from the results of the analysis for Task 1.

\% (x out of y) of cases where the correction of the self-categorization of the conclusion was necessary (comment from marci: this is 0 right now as we discussed, harry will go through them)

Nr. of analytical reproducibility checks:

```{r include=FALSE}
reproducibility_checks_data <-
  peer_eval |> 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_data |> 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") |> 
  summarise(n_reproducibility_checks = sum(n))
```

```{r include=FALSE}
reproducibility_checks_successful <-
  peer_eval |> 
  filter(any_code_mismatches != "(1) I didn’t try to execute it") |> 
  mutate(successful = case_when(
    any_code_mismatches %in% c("(2) I tried but didn’t manage to execute it", "(4) I executed it and I found mismatches") ~ FALSE,
    any_code_mismatches == "(3) I executed it and I found no mismatches" ~ TRUE,
    )
  ) |> 
  calculate_percentage(response_var = successful)
```

`r filter(reproducibility_checks_successful, successful) |> pull(percentage)`% (`r filter(reproducibility_checks_successful, successful) |> pull(n)` out of `r filter(reproducibility_checks_successful, successful) |> pull(N)` of the analytical reproducibility checks were successful

## How robust are the conclusions to analytical choices published in social sciences?

Do different analysts arrive at the same conclusions as the analysts of the original study?

### Task 1 Survey results

```{r include=FALSE}
# Distinct values of task1_categorisation
distinct(processed, task1_categorisation)
distinct(processed, task1_categorisation_plotting)

conclusions_main_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = simplified_paper_id, categorization_var = categorisation) |>
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(categorisation == "Same conclusion", percentage, NA), .desc = FALSE, .na_rm = TRUE)
  )

conclusions_main_robustness_data <- calculate_conclusion_robustness(data = processed, categorization_var = task1_categorisation_plotting)
```

In Task 1, the co-analysts were asked to conduct any statistical analysis to arrive to a single conclusion. Out of `r distinct(conclusions_main_robustness_data, N) |> pull(N)` re-analysed studies, the conclusions of `r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(n)` (`r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(n)`%) remained robust to independent re-analysis, so that all assigned co-analysts arrived at the same conclusion as reported in the article of the original study.

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-main-robustness
#| fig-cap: "The figure shows "

conclusion_main_robustness_plot <- plot_conclusion_robustness(conclusions_main_robustness_data, robust)

ggsave(here::here("figures/conclusion_main_robustness_plot.jpg"), conclusion_main_robustness_plot, dpi = 300)

conclusion_main_robustness_plot
```

@fig-conclusions-main shows the histotrophic display of the different and identical conclusions resulting from the re-analysis of each of the studies.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-main
#| fig-cap: "The figure shows the histrophic display of the type of conclusions resulting from the re-analysis of each study."

conclusion_main_plot <-
  plot_percentage(
    data = conclusions_main_data,
    grouping_var = simplified_paper_id,
    categorization_var = categorisation,
    y_lab = "Study number",
    x_lab = "Percentage of re-analyses",
    with_sum = FALSE,
    reverse = TRUE,
    rev_limits = FALSE
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 10),
    axis.title.x = element_text(size = 15),
    legend.justification = "left",
    legend.box = "horizontal",
    legend.position = "bottom",
    legend.text = element_text(size = 10)
  )

# Using a hacky solution to move legend under the Y axis
# conclusion_main_plot_wo_legend <- conclusion_main_plot + theme(legend.position = "none")
# legend <- cowplot::get_legend(conclusion_main_plot)
# conclusion_main_plot_w_legend <- cowplot::plot_grid(conclusion_main_plot_wo_legend, legend, nrow = 2, rel_heights = c(1, 0.05), rel_widths = c(1, 5))

ggsave(here::here("figures/conclusion_main_plot.jpg"), conclusion_main_plot, width = 9, height = 11.69, dpi = 300)

conclusion_main_plot
```

```{r include=FALSE}
conclusions_analysis_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  count(categorisation) |> 
  ungroup() |> 
  mutate(
    N = sum(n),
    freq = n / N,
    percentage = round(freq * 100, 2)
  )
```

Across all the re-analyses, `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(percentage)` % (`r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(N)`) of them arrived at the same conclusion; `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(N)`) to no effects, and `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(N)`) to opposite effect compared to the original conclusion.

### Inferential robustness
#### Inferential robustness by discipline

We were interested to see whether these results show a different pattern when inspecting them in different disciplines. @fig-discipline-robustness shows that for the major fields with more than 10 papers (Economics, Political Science, and Psychology) the pattern was comparably similar. We found no outstanding differences between the fields for the percentage of different and identical conclusions neither (see @fig-conclusions-discipline).

```{r include=FALSE}
conclusions_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = paper_discipline, categorization_var = categorisation)

conclusions_discipline_robustness_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  calculate_conclusion_robustness(grouping_var = paper_discipline, categorization_var = task1_categorisation_plotting)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-discipline-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by major fields (more than 10 papers)."

conclusions_discipline_robustness_plot <-
  plot_percentage(
    data = conclusions_discipline_robustness_data,
    categorization_var = robust,
    grouping_var = paper_discipline,
    with_labels = TRUE,
    x_lab = "Percentage of studies",
    y_lab = "Disciplines"
  )

ggsave(here::here("figures/conclusions_discipline_robustness_plot.jpg"), conclusions_discipline_robustness_plot, dpi = 300)

conclusions_discipline_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-discipline
#| fig-cap: "The figure shows percentage of identical, inconclusive, and different conclusion of the studies by major fields. The figure displays the count of re-analyses next to each field name."

conclusions_discipline_plot <-
  plot_percentage(
    data = conclusions_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = categorisation,
    with_labels = TRUE,
    x_lab = "Percentage of re-analyses",
    y_lab = "Disciplines"
  ) +
  ggplot2::theme(
    legend.text = ggplot2::element_text(size = 8)
  )

ggplot2::ggsave(here::here("figures/conclusions_discipline_plot.jpg"), conclusions_discipline_plot, dpi = 300)

conclusions_discipline_plot
```

#### Inferential robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type. @fig-studytype-robustness illustrates that nearly half of the results from experimental studies remained robust upon independent re-analysis, whereas only one-third of observational studies yielded robust conclusions. Moreover, @fig-conclusions-studytype indicates that, for both study types, the majority of the re-analyses reached the same conclusions as the original study.

```{r include=FALSE}
conclusions_studytype_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(categorisation = forcats::fct_relevel(
    categorisation,
    c("Same conclusion", "No effect/inconclusive", "Opposite effect")
  )) |>
  calculate_conclusion(grouping_var = experimental_or_observational,
                       categorization_var = categorisation) |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational))

conclusions_studytype_robustness_data <-
  processed |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational)) |>
  calculate_conclusion_robustness(grouping_var = experimental_or_observational,
                                  categorization_var = task1_categorisation_plotting) |>
  dplyr::ungroup()
```

```{r echo=FALSE, message=FALSE}
#| label: fig-studytype-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by study type (experimental or observational). The figure displays the count of re-analyses next to each field name."

conclusions_studytype_robustness_plot <-
  plot_percentage(
    data = conclusions_studytype_robustness_data,
    categorization_var = robust,
    grouping_var = experimental_or_observational,
    x_lab = "Percentage of studies",
    y_lab = "Study type",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_robustness_plot.jpg"), conclusions_studytype_robustness_plot, dpi = 300)

conclusions_studytype_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-studytype
#| fig-cap: "The figure shows percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by study type (experimental, observational)."

conclusions_studytype_plot <-
  plot_percentage(
    data = conclusions_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = categorisation,
    y_lab = "Study type",
    x_lab = "Percentage of re-analyses",
    with_labels = TRUE
  ) +
  ggplot2::theme(legend.text = element_text(size = 8))

ggplot2::ggsave(here::here("figures/conclusions_studytype_plot.jpg"), conclusions_studytype_plot, dpi = 300)

conclusions_studytype_plot
```

#### Inferential robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts. The following figure shows these results.

```{r include=FALSE}
conclusions_expertise_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  calculate_conclusion(grouping_var = expertise_self_rating, categorization_var = categorisation) |> 
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    ),
    expertise_self_rating = dplyr::case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  )
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-expertise
#| fig-cap: "The figure shows ... The figure does not display the bottom two categories where for each less than 3 responses were collected."

conclusions_expertise_plot <-
  plot_height(
    data = conclusions_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = categorisation,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    with_sum = TRUE,
    rev_limits = FALSE
  ) +
  theme(legend.text = element_text(size = 8))

ggsave(here::here("figures/conclusions_expertise_plot.jpg"), conclusions_expertise_plot, 
       height = 6, width = 8,
       dpi = 300)

conclusions_expertise_plot
```

#### Inferential robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. The following figure shows that for these results.

```{r include=FALSE}
count(processed, familiar_with_paper)

conclusions_familiarity_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |>
  mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    familiar_with_paper = as.factor(familiar_with_paper)
    ) |> 
  calculate_conclusion(grouping_var = familiar_with_paper, categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-familiarity
#| fig-cap: "The figure shows "

conclusions_familiarity_plot <- 
  plot_percentage(
    data = conclusions_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = categorisation,
    with_labels = TRUE,
    x_lab = "Percentage of re-analyses",
    y_lab = "Analyst familiarity with the paper") + 
  theme(
    legend.text = element_text(size = 8)
    )

ggsave(here::here("figures/conclusions_familiarity_plot.jpg"), conclusions_familiarity_plot, dpi = 300)

conclusions_familiarity_plot
```

#### Inferential robustness by the suitability of their self-judged analysis

```{r include=FALSE}
conclusions_suitability_data <- 
  processed |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
    ) |> 
  calculate_conclusion(grouping_var = confidence_in_approach, categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-suitability
#| fig-cap: "The figure shows "

conclusions_suitability_plot <-
  # TODO: shit name for a function replace or incorporate it with plot_percentage
  plot_height(
    data = conclusions_suitability_data,
    grouping_var = confidence_in_approach,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Number of re-analyses",
    x_lab = "Level of confidence with the suitability of the analysis",
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    # legend.text = ggplot2::element_text(size = 8),
    legend.position = "bottom",
    legend.box = "horizontal",
    axis.title = ggplot2::element_text(size = 10),
    legend.text = ggplot2::element_text(size = 8)
  )

ggplot2::ggsave(here::here("figures/conclusions_suitability_plot.jpg"), conclusions_suitability_plot, dpi = 300)

conclusions_suitability_plot
```

#### Inferential robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following figure shows that for that...,

```{r include=FALSE}
processed |>
  ggplot2::ggplot() +
  # Converting to natlog because of extreme outlier
  ggplot2::aes(x = log(reanalysis_model_sample_size)) +
  ggplot2::geom_histogram()

processed |> 
  dplyr::count(is.na(reanalysis_model_sample_size))
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-samplesize
#| fig-cap: "The figure shows "

conclusions_samplesize_plot <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_model_sample_size) |>
  # TODO: There are missing sample size values what to do with them?
  dplyr::filter(!is.na(reanalysis_model_sample_size)) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    )
  ) |>
  plot_rain(
    grouping_var = categorisation,
    response_var = reanalysis_model_sample_size,
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000),
    x_lab = "Direction of the re-analysis conclusion\ncompared to the original effect",
    y_lab = "Sample size"
  )

ggplot2::ggsave(here::here("figures/conclusions_samplesize_plot.jpg"), conclusions_samplesize_plot, dpi = 300)

conclusions_samplesize_plot
```

#### How inferentially robust are statistical findings published in social sciences to analytical choices?

A main question of our study was whether different analysts arrive at the same effect estimates (+/- 0.05 Cohen's d) as the analyst of the original study?

```{r include=FALSE}
# Number of cases where the original effect size is missing
missing_original_n <-
  processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(is.na(original_cohens_d)) |> 
  dplyr::filter(`is.na(original_cohens_d)`) |> 
  dplyr::pull(n)

# Check if there are cases where there is missing original effect size on the paper level but not for every analysis
# Would mean that there is a mistake in the merging code
processed |> 
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::group_by(paper_id) |> 
  dplyr::mutate(
    number_of_analysis = dplyr::n() 
  ) |> 
  dplyr::filter(is.na(original_cohens_d)) |> 
  mutate(
    number_of_analysis_after_filtering = dplyr::n()
  ) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(
    match = dplyr::if_else(number_of_analysis == number_of_analysis_after_filtering, TRUE, FALSE)
  )

# Number of cases where the reanalysed effect size is missing
missing_reanalysis_n <-
  processed |> 
  dplyr::count(is.na(reanalysis_cohens_d)) |> 
  dplyr::filter(`is.na(reanalysis_cohens_d)`) |> 
  dplyr::pull(n)

# Are there any cases where the original effect size is present but the reanalyzed is not and vica versa?
processed |> 
  dplyr::select(original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::mutate(
    both_present_or_missing = is.na(original_cohens_d) == is.na(reanalysis_cohens_d)
  ) |> 
  dplyr::count(both_present_or_missing)

# Check if there are any cases where everything is missing
processed |> 
  dplyr::group_by(paper_id) |>
  dplyr::mutate(
    both_missing = all(is.na(original_cohens_d) & is.na(reanalysis_cohens_d))
  ) |>
  dplyr::ungroup() |> 
  dplyr::count(both_missing)
  
# Preparing the data for the plot
# TODO: they want two colors and color legend with fixed position
reanalysis_data <-
  processed |>
  dplyr::select(simplified_paper_id,
                analyst_id,
                reanalysis_cohens_d,
                original_cohens_d) |>
  dplyr::group_by(simplified_paper_id) |>
  dplyr::rename(effect_size = reanalysis_cohens_d) |>
  dplyr::mutate(effect_size_type = paste0("re-analysis_0", row_number()),) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    # Put missing values at the end
    original_cohens_d = dplyr::if_else(is.na(original_cohens_d), -Inf, original_cohens_d),
    simplified_paper_id = forcats::fct_reorder(.f = simplified_paper_id,
                                               .x = original_cohens_d,
                                               .fun = function(x) median(x),
                                               .na_rm = FALSE)
  )

original_data <-
  processed |>
  dplyr::distinct(simplified_paper_id, original_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = forcats::fct_reorder(simplified_paper_id,
                                               original_cohens_d,
                                               .na_rm = FALSE)
  )

excluded_es <-
  dplyr::filter(reanalysis_data, effect_size > 10 | effect_size < -10) |> 
  mutate(excluded = glue::glue("study {simplified_paper_id}: {effect_size}")) |> 
  summarise(excluded_list = glue::glue_collapse(excluded, sep = "; "))
```

The figure does not show `r dplyr::filter(reanalysis_data, effect_size > 10 | effect_size < -10) |> nrow()` (`r dplyr::pull(excluded_es, excluded_list)`) re-analyzed effect sizes which are over 10 or smaller than -10 Cohen's d.

```{r echo=FALSE, warning=FALSE}
#| label: fig-effect-main
#| fig-cap: "The figure shows "

# Colors
# color_vector <- setNames(c("#F8766D", "#CD9600", "#7CAE00", "#00BE67", "#00BFC4", "#00A9FF", "#C77CFF"), paste0("re-analysis_", sprintf("%02d", 1:7)))

effect_main_plot <-
  reanalysis_data |> 
  # TODO: what to do with huge cohens ds?
  dplyr::filter(effect_size <= 10 & effect_size >= -10) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    y = simplified_paper_id,
    x = effect_size,
    # color = effect_size_type
  ) +
  ggplot2::geom_point(
    shape = 16,
    color = viridis::viridis(5)[[3]],
    # size = 5
    ) +
  # ggplot2::scale_color_manual(values = color_vector) +
  ggplot2::geom_pointrange(
    data = original_data,
    ggplot2::aes(
      x = original_cohens_d,
      xmin = tolarence_region_lower,
      xmax = tolarence_region_upper,
      alpha = 0.8
      ),
    show.legend = FALSE,
    color = "black",
    shape = 15,
    # size = 2
    ) +
  labs(
    x = "Effect size in Cohen's d",
    y = "Study number"
  ) +
  ggplot2::guides(color = "none") +
  ggplot2::theme(
    axis.ticks = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(),
    plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10, "pt"),
    panel.grid = ggplot2::element_line(color = "lightgray"),
    panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.title = ggplot2::element_text(size = 20),
    axis.text.y = ggplot2::element_text(size = 10),
    axis.text.x = ggplot2::element_text(size = 13)
    # axis.text.y=element_text(margin = margin(1, unit = "cm"), vjust =1.5)
    )

# TODO: for some reason tolerance region is not showing on saved plot... why?
ggplot2::ggsave(here::here("figures/effect_main_plot.jpg"), plot = effect_main_plot,
       width = 12, height = 13,
       dpi = 300)

effect_main_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-all
#| fig-cap: "The figure shows "

effect_region_all_data <- 
  processed |> 
  calculate_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )
  
# TODO: replace this function and delete it from utils
effect_region_all_plot <- plot_tolarence_region(data = effect_region_all_data, grouping_var = simplified_paper_id, y_lab = "Study number", x_lab = "Percentage of reanalysis results") +
  ggplot2::theme(
    axis.title = ggplot2::element_text(size = 15),
    legend.text = ggplot2::element_text(size = 13),
    axis.text.x = ggplot2::element_text(size = 13)
  )

ggplot2::ggsave(here::here("figures/effect_region_all_plot.jpg"), effect_region_all_plot, width = 8.27, height = 11.69, dpi = 300)

effect_region_all_plot
```

```{r include=FALSE}
ind_within_tol_reg <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d >= tolarence_region_lower &
        reanalysis_cohens_d <= tolarence_region_upper ~ "Within tolerance region",
      reanalysis_cohens_d < tolarence_region_lower |
        reanalysis_cohens_d > tolarence_region_upper ~ "Outside of tolerance region",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(
      is_within_region,
      levels = c("Within tolerance region", "Outside of tolerance region")
    )
  ) |> 
  # Exclude missing tolerance region decisions
  dplyr::filter(!is.na(is_within_region)) |> 
  calculate_percentage(is_within_region)
  
within_tolerance_region <-
  effect_region_all_data |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & relative_frequency == 1), "Inferentially robust", "Inferentially not Robust")
    ) |> 
  dplyr::ungroup() |> 
  dplyr::count(robust) |> 
  dplyr::mutate(
    N = sum(n),
    relative_frequency = n / N,
    percentage = round(relative_frequency * 100)
    )
```

Here, we were interested in what percentage of the new effect sizes were beyond the tolerance region (+/- 0.05 Cohen's d). We found that `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(percentage)`% (`r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(n)` out of `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(N)`) of the studies contained at least one re-analysis result where the effect size was beyond the tolerance region (+/- 0.05 Cohen's d) of the result of the original study. Out of the `r dplyr::distinct(ind_within_tol_reg, N) |> dplyr::pull(N)` reanalyses effect sizes with available tolerance regions `r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(percentage)`% (`r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(n)`) were outside of the tolerance region.

##### Estimate robustness by discipline

We were interested to see whether these robustness results show a different pattern when inspecting them in different fields. The following figure shows that for the major fields (\>=10 studies).

```{r include=FALSE, warning=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(paper_discipline) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-discipline
#| fig-cap: "The figure shows "

effect_region_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |>
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  # Exclude missing values
  calculate_tolerance_region(grouping_var = paper_discipline, drop_missing = TRUE)

effect_region_discipline_plot <-
  plot_percentage(
    data = effect_region_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = is_within_region,
    x_lab = "Percentage of re-analysis results",
    y_lab = "Disciplines",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_discipline_plot.jpg"), effect_region_discipline_plot, dpi = 300)

effect_region_discipline_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-discipline
#| fig-cap: "The figure shows "

effect_robustness_discipline_data <-
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  calculate_estimate_range(grouping_var = paper_discipline)

effect_robustness_discipline_plot <-
  plot_rain(data = effect_robustness_discipline_data,
            grouping_var = paper_discipline,
            response_var = estimate_range,
            x_lab = "Disciplines",
            y_lab = "Effect size estimate range in Cohen's d",
            trans = "log10",
            breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60))

ggplot2::ggsave(here::here("figures/effect_robustness_discipline_plot.jpg"), effect_robustness_discipline_plot, dpi = 300)

effect_robustness_discipline_plot 
```

##### Estimate robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type.

```{r include=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(experimental_or_observational) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-studytype
#| fig-cap: "The figure shows "

effect_region_studytype_data <- 
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_tolerance_region(grouping_var = experimental_or_observational, drop_missing = TRUE)

effect_region_studytype_plot <-
  plot_percentage(
    data = effect_region_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = is_within_region,
    x_lab = "Percentage of re-analysis results",
    y_lab = "Study type",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_studytype_plot.jpg"), effect_region_studytype_plot, dpi = 300)

effect_region_studytype_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-studytype
#| fig-cap: "The figure shows "

effect_robustness_studytype_data <-
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_estimate_range(grouping_var = experimental_or_observational)

effect_robustness_studytype_plot <- plot_rain(
  data = effect_robustness_studytype_data,
  grouping_var = experimental_or_observational,
  response_var = estimate_range,
  x_lab = "Study type",
  y_lab = "Effect size estimate range in Cohen's d",
  trans = "log10",
  breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)
)

ggplot2::ggsave(here::here("figures/effect_robustness_studytype_plot.jpg"), effect_robustness_studytype_plot, dpi = 300)

effect_robustness_studytype_plot 
```

##### Estimate robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts.

```{r include=FALSE}
processed |> 
  dplyr::distinct(expertise_self_rating)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-expertise
#| fig-cap: "The figure shows "

effect_region_expertise_data <- 
  processed |> 
  dplyr::mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(expertise_self_rating, levels = c(
      "1\n(Beginner)",
      as.character(2:9),
      "10\n(Expert)")
    )
  ) |> 
  calculate_tolerance_region(grouping_var = expertise_self_rating, drop_missing = TRUE)

effect_region_expertise_plot <-
  plot_height(
    data = effect_region_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    rev_limits = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_expertise_plot.jpg"), effect_region_expertise_plot, dpi = 300,
                width = 8, height = 6
                )

effect_region_expertise_plot
```

##### Estimate robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-effect-region-familiarity shows that for these results.

```{r echo=FALSE, message=FALSE, message=FALSE}
#| label: fig-effect-region-familiarity
#| fig-cap: "The figure shows..."

effect_region_familiarity_data <- calculate_tolerance_region(data = processed, grouping_var = familiar_with_paper, drop_missing = TRUE)

effect_region_familiarity_plot <-
  plot_percentage(
    data = effect_region_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = is_within_region,
    x_lab = "Percentage of re-analysis estimates in Cohen's d",
    y_lab = "Familiar with the paper",
    with_labels = TRUE
  )

ggplot2::ggsave(here::here("figures/effect_region_familiarity_plot.jpg"), effect_region_familiarity_plot, dpi = 300)

effect_region_familiarity_plot
```

##### Estimate robustness by the suitability of their self-judged analysis

```{r include=FALSE}
processed |> 
  dplyr::distinct(confidence_in_approach)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-suitability
#| fig-cap: "The figure shows "

effect_region_suitability_data <- calculate_tolerance_region(data = processed, grouping_var = confidence_in_approach, drop_missing = TRUE) |> 
  dplyr::mutate(
        confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
  )

effect_region_suitability_plot <-
  plot_height(
    data = effect_region_suitability_data,
    grouping_var = confidence_in_approach,
    categorization_var = is_within_region,
    x_lab = "Number of re-analyses",
    y_lab = "Level of confidence with the suitability of the analysis",
    with_labels = TRUE,
    rev_limits = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_suitability_plot.jpg"), effect_region_suitability_plot, dpi = 300, 
                width = 8, height = 6
                )

effect_region_suitability_plot
```

##### Estiamte robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following @fig-samplesize-region shows that for that..., 

```{r echo=FALSE, message=FALSE}
#| label: fig-samplesize-region
#| fig-cap: "The figure shows how the sample size influences whether the re-analysis effect sizes were within the tolerance region. For the figure we did not inlcude: those studies were the original effect sizes were missing, and cases where the re-analysis effect size or sample size were missing."

samplesize_region_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_model_sample_size,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d <= tolarence_region_lower |
        reanalysis_cohens_d >= tolarence_region_upper ~ "No",
      reanalysis_cohens_d >= tolarence_region_lower |
        reanalysis_cohens_d <= tolarence_region_upper ~ "Yes",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(is_within_region, levels = c("Yes", "No"))
  ) |> 
  dplyr::filter(
    !is.na(original_cohens_d),
    !is.na(reanalysis_model_sample_size),
    is_within_region != "Missing")


samplesize_region_plot <-
  plot_rain(
    data = samplesize_region_data,
    grouping_var = is_within_region,
    response_var = reanalysis_model_sample_size,
    y_lab = "Sample size",
    x_lab = "Is the re-analysis effect size within the tolerance region?",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  )

ggplot2::ggsave(here::here("figures/effect_region_samplesize_plot.jpg"), samplesize_region_plot, dpi = 300)

samplesize_region_plot
```

